{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haplotype analysis *Rdl*\n",
    "\n",
    "## Input\n",
    "\n",
    "Input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "outdir   = \"haplotype_analysis_output\"\n",
    "outcode  = \"rdo_hapnet\"\n",
    "\n",
    "# gene of interest\n",
    "chrom     = \"2L\"\n",
    "l_nom     = \"rdl\"    # nom loci\n",
    "loc_start = 25363652 # start gene\n",
    "loc_end   = 25434556 # end gene\n",
    "popc      = \"population\"\n",
    "\n",
    "# inversion 2La\n",
    "inv_start = 20524058 # interval 20524058 - 20528089\n",
    "inv_end   = 42165532 # interval 42165182 - 42165532\n",
    "\n",
    "# retain all data between these coordinates, to ease up on memory\n",
    "ret_start  = loc_start-1e6\n",
    "ret_end    = loc_end+1e6\n",
    "ret_start  = 0  \n",
    "ret_end    = 50000000\n",
    "\n",
    "# min frq to retain minor allele\n",
    "minfrq     = 0.05\n",
    "# input data phase2\n",
    "p2_metasam_fn = \"data/samples_p2.meta.txt\"\n",
    "p2_hapcall_fn = \"/home/xavi/Documents/VariationAg1k/data/phase2.AR1/haplotypes/zarr2/ag1000g.phase2.ar1.samples/\"  #### EDIT THIS \n",
    "p2_accessi_fn = \"/home/xavi/Documents/VariationAg1k/data/phase2.AR1/accessibility/accessibility.h5\"                #### EDIT THIS\n",
    "p2_popc       = popc\n",
    "p2_popl       = [\"AOcol\",\"BFcol\",\"BFgam\",\"CIcol\",\"CMgam\",\"FRgam\",\"GAgam\",\"GHcol\",\"GHgam\",\"GM\",\"GNcol\",\"GNgam\",\"GQgam\",\"GW\",\"KE\",\"UGgam\"]\n",
    "\n",
    "# outgroup populations\n",
    "ou_species    = [\"arab\",\"quad\",\"meru\",\"mela\"]\n",
    "ou_hapcall_fl = [\"/home/xavi/dades/Variation/phase1.AR3_Fontaine_AltSps/haplotypes/arab_ref_hc_vqsr_cnvrt_sort.zarr/\",       #### EDIT THIS\n",
    "                 \"/home/xavi/dades/Variation/phase1.AR3_Fontaine_AltSps/haplotypes/quad_ref_hc_vqsr_cnvrt_sort.zarr/\",       #### EDIT THIS\n",
    "                 \"/home/xavi/dades/Variation/phase1.AR3_Fontaine_AltSps/haplotypes/meru_hc_vqsr_cnvrt_sort.zarr/\",           #### EDIT THIS\n",
    "                 \"/home/xavi/dades/Variation/phase1.AR3_Fontaine_AltSps/haplotypes/mela_ref_hc_vqsr_good_cnvrt_sort.zarr/\"]  #### EDIT THIS\n",
    "ou_metasam_fl = [\"data/samples.metaara.txt\",\n",
    "                 \"data/samples.metaqua.txt\",\n",
    "                 \"data/samples.metamer.txt\",\n",
    "                 \"data/samples.metamel.txt\"]\n",
    "ou_popc       = popc\n",
    "\n",
    "# karyotype data\n",
    "kary_fn    = \"data/kt_2la.karyotype_with_outgroups.csv\"\n",
    "# accessibility phase2\n",
    "accessi_fn = \"/home/xavi/Documents/VariationAg1k/data/phase2.AR1/accessibility/accessibility.h5\" #### EDIT THIS\n",
    "# snp effects\n",
    "snpeff_fn  = \"/home/xavi/Documents/VariationAg1k/data/phase2.AR1/snpeff/zarr2/\"                  #### EDIT THIS\n",
    "# gff\n",
    "gffann_fn  = \"data/Anopheles-gambiae-PEST_BASEFEATURES_AgamP4.9.gff3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zarr\n",
    "import pandas as pd\n",
    "import allel\n",
    "import h5py\n",
    "import warnings\n",
    "import scipy\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from mlxtend.evaluate import permutation_test\n",
    "warnings.simplefilter('ignore')\n",
    "import random\n",
    "\n",
    "%run scripts_hapclust/hapclust.py\n",
    "\n",
    "# load plot settings\n",
    "sns.set(context=\"notebook\",style=\"ticks\",\n",
    "        font_scale=1,font=\"Arial\",palette=\"bright\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color palette for networks (must be named!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary of pop colors\n",
    "pop_colors = {\n",
    " 'AOcol' : \"forestgreen\",\n",
    " 'BFcol' : \"red\",\n",
    " 'BFgam' : \"springgreen\",\n",
    " 'CIcol' : \"dodgerblue\",\n",
    " 'CMgam' : \"orange\",\n",
    " 'FRgam' : \"magenta\",\n",
    " 'GAgam' : \"blueviolet\",\n",
    " 'GHcol' : \"yellowgreen\",\n",
    " 'GHgam' : \"deeppink\",\n",
    " 'GM'    : \"cyan\",\n",
    " 'GNcol' : \"peru\",\n",
    " 'GNgam' : \"turquoise\",\n",
    " 'GQgam' : \"sienna\",\n",
    " 'GW'    : \"gold\",\n",
    " 'KE'    : \"steelblue\",\n",
    " 'UGgam' : \"violet\",\n",
    " 'CMara' : \"slategray\",\n",
    " 'BFara' : \"slategray\",\n",
    " 'TZara' : \"slategray\",\n",
    " 'ZMqua' : \"gray\",\n",
    " 'SAmer' : \"gray\",\n",
    " 'KEmer' : \"gray\",\n",
    " 'CMmel' : \"gray\",\n",
    "}\n",
    "pos_colors = {\n",
    " 'AOcol' : \"red\",\n",
    " 'BFcol' : \"red\",\n",
    " 'BFgam' : \"green\",\n",
    " 'CIcol' : \"red\",\n",
    " 'CMgam' : \"green\",\n",
    " 'FRgam' : \"green\",\n",
    " 'GAgam' : \"green\",\n",
    " 'GHcol' : \"red\",\n",
    " 'GHgam' : \"green\",\n",
    " 'GM'    : \"blue\",\n",
    " 'GNcol' : \"red\",\n",
    " 'GNgam' : \"green\",\n",
    " 'GQgam' : \"green\",\n",
    " 'GW'    : \"blue\",\n",
    " 'KE'    : \"blue\",\n",
    " 'UGgam' : \"green\",\n",
    " 'CMara' : \"orange\",\n",
    " 'BFara' : \"orange\",\n",
    " 'TZara' : \"orange\",\n",
    " 'ZMqua' : \"gray\",\n",
    " 'SAmer' : \"slategray\",\n",
    " 'KEmer' : \"lightgray\",\n",
    " 'CMmel' : \"steelblue\",\n",
    "}\n",
    "\n",
    "var_colors = dict({\n",
    "    -1 : \"orange\",\n",
    "    0  : \"lightgray\",\n",
    "    1  : \"deepskyblue\",\n",
    "    2  : \"blue\",\n",
    "    3  : \"violet\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "### Phase2 variants\n",
    "\n",
    "Population and sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "* Samples     =  1142\n",
      "* Populations =  {'UGgam', 'GW', 'FRgam', 'BFgam', 'GAgam', 'GNgam', 'GHgam', 'GQgam', 'GNcol', 'KE', 'BFcol', 'GM', 'CIcol', 'CMgam', 'GHcol', 'AOcol'}\n",
      "population\n",
      "AOcol     78\n",
      "BFcol     75\n",
      "BFgam     92\n",
      "CIcol     71\n",
      "CMgam    297\n",
      "FRgam     24\n",
      "GAgam     69\n",
      "GHcol     55\n",
      "GHgam     12\n",
      "GM        65\n",
      "GNcol      4\n",
      "GNgam     40\n",
      "GQgam      9\n",
      "GW        91\n",
      "KE        48\n",
      "UGgam    112\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load samples list with sample code, groupings, locations etc.\n",
    "p2_samples_df   = pd.read_csv(p2_metasam_fn, sep='\\t')\n",
    "p2_samples_bool = (p2_samples_df[p2_popc].isin(p2_popl).values)\n",
    "p2_samples      = p2_samples_df[p2_samples_bool]\n",
    "p2_samples.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# indexed dictionary of populations\n",
    "p2_popdict = dict()\n",
    "for popi in p2_popl: \n",
    "    p2_popdict[popi]  = p2_samples[p2_samples[p2_popc] == popi].index.tolist()\n",
    "\n",
    "# add an extra population composed of all other locations\n",
    "p2_popdict[\"all\"] = []\n",
    "for popi in p2_popl:\n",
    "    p2_popdict[\"all\"] = p2_popdict[\"all\"] + p2_popdict[popi]\n",
    "\n",
    "\n",
    "# report\n",
    "print(\"Data:\")\n",
    "print(\"* Samples     = \", p2_samples.shape[0])\n",
    "print(\"* Populations = \", set(p2_samples[p2_popc]))\n",
    "print(p2_samples.groupby((\"population\")).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phased variants and genotypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variants phased...\n",
      "(8906423,)\n",
      "Genotypes phased...\n"
     ]
    }
   ],
   "source": [
    "# declare objects with variant data\n",
    "p2_hapcall   = zarr.open(p2_hapcall_fn)\n",
    "# variants of genotypes\n",
    "print(\"Variants phased...\")\n",
    "p2_hapcall_var = p2_hapcall[chrom][\"variants\"]\n",
    "p2_hapvars = allel.VariantChunkedTable(p2_hapcall_var,names=[\"POS\",\"REF\",\"ALT\"],index=\"POS\") \n",
    "print(p2_hapvars.shape)\n",
    "# genotype data\n",
    "print(\"Genotypes phased...\")\n",
    "p2_hapcall_hap = p2_hapcall[chrom][\"calldata\"][\"genotype\"]\n",
    "p2_haploty     = allel.GenotypeChunkedArray(p2_hapcall_hap) \n",
    "p2_haploty     = p2_haploty.subset(sel1=p2_samples_bool)\n",
    "print(p2_haploty.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsetting step specific to this analysis: remove variants outside of this region to ease up on memory. If this block is excluded, everything works as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phased\n",
    "print(\"Subset phased variants to region...\")\n",
    "pos_booh   = np.logical_and(p2_hapvars[\"POS\"] >= ret_start, p2_hapvars[\"POS\"] <= ret_end)\n",
    "p2_hapvars = p2_hapvars.compress(pos_booh)\n",
    "p2_haploty = p2_haploty.subset(sel0=pos_booh)\n",
    "print(p2_haploty.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does this start and end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Phased variants:\",p2_hapvars[\"POS\"][0],\"-\",p2_hapvars[\"POS\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outgroups\n",
    "\n",
    "Loads one outgroup, removes indels (duplicated variant positions) and subsets phase2 to include variants present in this outgroup. Then, loads outgroup genotypes and subsets them to remove indels and fit phase2. Then, loads the second outgroup and performs the same task. Thus, at each iteration, less and less variants remain (hopefully not too many are lost; worst offenders are `chri` and `epir`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_indels(gt, ref, alt, pos):\n",
    "    \"\"\"This removes indels from the Fontaine data\n",
    "    https://github.com/SeanTomlinson30/phd-ops/blob/master/218-arabiensis-aim/20181127-fontaine-data-munging.ipynb\"\"\"\n",
    "    # Utility\n",
    "    mylen = np.vectorize(len)\n",
    "    # ALT\n",
    "    count_alt = mylen(alt)\n",
    "    alt_count = np.sum(count_alt, axis=1)\n",
    "    # This array returns true at each position if the alt is fixed for the reference or has only one alternate allele.\n",
    "    is_ref_or_snp = alt_count <= 1\n",
    "    # REF Let's pull out the positions where it's a single base\n",
    "    length_of_ref = mylen(ref) <= 1 \n",
    "    # return the logical and of these 2 arrays this removes the second instance of a duplicated position\n",
    "    is_not_dup = np.concatenate([[True], np.diff(pos) > 0])\n",
    "    # this is the final bool array to filter the data with, and results in positions that are not multiallelic not containing indel and not containing duplicated positions\n",
    "    bool_mask = length_of_ref & is_ref_or_snp & is_not_dup\n",
    "    return(bool_mask)\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_haploty = p2_haploty\n",
    "oc_hapvars = p2_hapvars\n",
    "\n",
    "for outn,outi in enumerate(ou_species):\n",
    "    \n",
    "    print(\"# p2 phased genotypes remaining: %i\" % oc_haploty.shape[0])\n",
    "    \n",
    "    # Variants\n",
    "    print(\"Variants phased %s...\" % outi)\n",
    "    ou_hapcall = zarr.open(ou_hapcall_fl[outn])\n",
    "    ou_hapvars = allel.VariantChunkedTable(ou_hapcall[chrom][\"variants\"],names=[\"POS\",\"REF\",\"ALT\"],index=\"POS\")\n",
    "\n",
    "    # retain positions in inversion & remove indels\n",
    "    print(\"Remove indel phased %s...\" % outi)\n",
    "    pos_noindeh = remove_indels(gt = ou_hapcall[chrom][\"calldata\"][\"GT\"], ref = ou_hapcall[chrom][\"variants\"][\"REF\"], alt = ou_hapcall[chrom][\"variants\"][\"ALT\"], pos = ou_hapvars[\"POS\"])\n",
    "    ou_hapvars  = ou_hapvars[:][pos_noindeh]\n",
    "    \n",
    "    # subset phase2 to fit phase1\n",
    "    print(\"Subset phased phase2 to %s...\" % outi)\n",
    "    is_p2_ih_ou = np.isin(oc_hapvars[\"POS\"],test_elements=ou_hapvars[\"POS\"])\n",
    "    oc_haploty  = oc_haploty.compress((is_p2_ih_ou))\n",
    "    oc_hapvars  = oc_hapvars.compress((is_p2_ih_ou))\n",
    "    print(oc_haploty.shape)\n",
    "    \n",
    "    # genotype data\n",
    "    print(\"Genotypes phased %s...\" % outi)\n",
    "    ou_haploty     = allel.GenotypeChunkedArray(ou_hapcall[chrom][\"calldata\"][\"GT\"]).compress(pos_noindeh)\n",
    "    \n",
    "    # retain positions available in phase2\n",
    "    print(\"Subset phased %s to phase2...\" % outi)\n",
    "    ih_ou_in_p2 = np.isin(ou_hapvars[\"POS\"],test_elements=oc_hapvars[\"POS\"])\n",
    "    ou_haploty  = ou_haploty.compress(ih_ou_in_p2)\n",
    "    print(ou_haploty.shape)\n",
    "\n",
    "    # add new genotypes to phase2\n",
    "    print(\"Merge phased %s into phase2...\" % outi)\n",
    "    oc_haploty = oc_haploty.concatenate(ou_haploty,axis=1)\n",
    "    print(oc_haploty.shape)\n",
    "    \n",
    "    del ou_haploty\n",
    "    del ou_hapvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand phase of phased variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recast haplotypes: drop ploidy\n",
    "print(\"Expand phase haplotypes...\")\n",
    "oc_haploty_hap = oc_haploty.to_haplotypes()\n",
    "print(oc_haploty_hap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge sample data\n",
    "\n",
    "Merge metadata files, with sample codes, species and populations, for variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cast sample metadata...\")\n",
    "oc_samples = pd.DataFrame(data={\n",
    "    \"ox_code\"    :  p2_samples[\"ox_code\"].values.tolist() + \n",
    "                    list(itertools.chain.from_iterable([ pd.read_csv(ou_metasam_fl[n], sep='\\t')[\"ox_code\"].values.tolist()  for n,i in enumerate(ou_species) ])),\n",
    "    \"species\"    :  p2_samples[\"m_s\"].values.astype(str).tolist() + \n",
    "                    list(itertools.chain.from_iterable([ pd.read_csv(ou_metasam_fl[n], sep='\\t')[\"species\"].values.tolist()  for n,i in enumerate(ou_species) ])),\n",
    "    \"population\" :  p2_samples[p2_popc].values.tolist()   + \n",
    "                    list(itertools.chain.from_iterable([ pd.read_csv(ou_metasam_fl[n], sep='\\t')[ou_popc].values.tolist()  for n,i in enumerate(ou_species) ]))\n",
    "})\n",
    "print(oc_samples.shape)\n",
    "\n",
    "# rename species...\n",
    "oc_samples[\"species\"].values[oc_samples[\"species\"].values == \"M\"]   = \"col\"\n",
    "oc_samples[\"species\"].values[oc_samples[\"species\"].values == \"S\"]   = \"gam\"\n",
    "oc_samples[\"species\"].values[oc_samples[\"species\"].values == \"M/S\"] = \"gamcol\"\n",
    "oc_samples[\"species\"].values[oc_samples[\"species\"].values == \"M-S\"] = \"gamcol\"\n",
    "oc_samples[\"species\"].values[oc_samples[\"species\"].values == \"nan\"] = \"gamcol\"\n",
    "\n",
    "# obtain full population & species list\n",
    "oc_popl = np.unique(oc_samples[\"population\"].values)\n",
    "oc_spsl = np.unique(oc_samples[\"species\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge metadata files, with sample codes, species and populations, for expanded phased variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cast sample metadata...\")\n",
    "oc_sampleh = pd.DataFrame(data={\n",
    "    \"ox_code\"    :  list(itertools.chain(*[[ s + 'a', s + 'b'] for s in oc_samples[\"ox_code\"].values.tolist()])),    # takes col from oc_samples and duplicates it, a/b\n",
    "    \"species\"    :  list(itertools.chain(*[[ s      , s      ] for s in oc_samples[\"species\"].values.tolist()])),\n",
    "    \"population\" :  list(itertools.chain(*[[ s      , s      ] for s in oc_samples[\"population\"].values.tolist()]))\n",
    "})\n",
    "print(oc_sampleh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries of populations and species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Population dict...\")\n",
    "oc_popdict = dict()\n",
    "for popi in oc_popl: \n",
    "    oc_popdict[popi]  = oc_samples[oc_samples[\"population\"] == popi].index.tolist()\n",
    "oc_popdict[\"all\"] = []\n",
    "for popi in oc_popl:\n",
    "    oc_popdict[\"all\"] = oc_popdict[\"all\"] + oc_popdict[popi]\n",
    "\n",
    "print(\"Population dict phased...\")\n",
    "oc_popdich = dict()\n",
    "for popi in oc_popl: \n",
    "    oc_popdich[popi]  = oc_sampleh[oc_sampleh[\"population\"] == popi].index.tolist()\n",
    "oc_popdich[\"all\"] = []\n",
    "for popi in oc_popl:\n",
    "    oc_popdich[\"all\"] = oc_popdich[\"all\"] + oc_popdich[popi]\n",
    "\n",
    "print(\"Species dict...\")\n",
    "oc_spsdict = dict()\n",
    "for spsi in oc_spsl: \n",
    "    oc_spsdict[spsi]  = oc_samples[oc_samples[\"species\"] == spsi].index.tolist()\n",
    "    \n",
    "print(\"Species dict phased...\")\n",
    "oc_spsdich = dict()\n",
    "for spsi in oc_spsl: \n",
    "    oc_spsdich[spsi]  = oc_sampleh[oc_sampleh[\"species\"] == spsi].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allele counts\n",
    "\n",
    "Using both dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Genotypes phased to allele counts (population)...\")\n",
    "oc_hapalco_pop = oc_haploty.count_alleles_subpops(subpops=oc_popdict)\n",
    "print(oc_hapalco_pop.shape)\n",
    "\n",
    "print(\"Haplotypes phased to allele counts (population)...\")\n",
    "oc_hapalco_hap_pop = oc_haploty_hap.count_alleles_subpops(subpops=oc_popdich)\n",
    "print(oc_hapalco_hap_pop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define which phased variants to retain from phase2 (all other datasets will be recast to fit this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data: segregating alleles, biallelic and no singletons\n",
    "print(\"Filters phased p2+ou...\")\n",
    "oc_is_seg_h    = oc_hapalco_hap_pop[\"all\"].is_segregating()[:] # segregating\n",
    "oc_is_nosing_h = oc_hapalco_hap_pop[\"all\"][:,:2].min(axis=1)>2 # no singletons\n",
    "\n",
    "# subset phase2 to seg, nosing, biallelic & outgroup size\n",
    "oc_hapvars_seg         = oc_hapvars.compress((oc_is_seg_h[:] & oc_is_nosing_h[:]))\n",
    "oc_haploty_seg         = oc_haploty.compress((oc_is_seg_h[:] & oc_is_nosing_h[:]))\n",
    "oc_hapalco_pop_seg     = oc_hapalco_pop.compress((oc_is_seg_h[:] & oc_is_nosing_h[:]))\n",
    "oc_haploty_hap_seg     = oc_haploty_hap.compress((oc_is_seg_h[:] & oc_is_nosing_h[:]))\n",
    "oc_hapalco_hap_pop_seg = oc_hapalco_hap_pop.compress((oc_is_seg_h[:] & oc_is_nosing_h[:]))\n",
    "\n",
    "# report\n",
    "print(oc_haploty_seg.shape,\"/\", oc_haploty.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other data\n",
    "\n",
    "Accessibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessibility\n",
    "print(\"Load accessibility array...\")\n",
    "accessi_df  = h5py.File(accessi_fn,mode=\"r\")\n",
    "accessi_arr = accessi_df[chrom][\"is_accessible\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gene annotations GFF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gff\n",
    "def geneset_gff(geneset):\n",
    "    items = []\n",
    "    for n in geneset.dtype.names:\n",
    "        v = geneset[n]\n",
    "        if v.dtype.kind == 'S':\n",
    "            v = v.astype('U')\n",
    "        items.append((n, v))\n",
    "    return pd.DataFrame.from_items(items)\n",
    "%run scripts_printtranscripts/allel_printtranscripts_24gen18.py\n",
    "\n",
    "geneset = allel.FeatureTable.from_gff3(gffann_fn,attributes=['ID', 'Parent'])\n",
    "geneset = geneset_gff(geneset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneset[geneset[\"Parent\"] == \"AGAP006028\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "print(\"Load variant effects...\")\n",
    "snpeff_zr          = zarr.open(snpeff_fn)\n",
    "oc_genveff         = allel.VariantChunkedTable(snpeff_zr[chrom][\"variants\"],names=[\"POS\",\"ANN\"],index=\"POS\") # variant effects\n",
    "is_ef_in_oc        = np.isin(oc_genveff[\"POS\"],test_elements=oc_hapvars_seg[\"POS\"])\n",
    "oc_genveff_seg_ann = oc_genveff[\"ANN\"][:][is_ef_in_oc]\n",
    "oc_genveff_seg_pos = oc_genveff[\"POS\"][:][is_ef_in_oc]\n",
    "\n",
    "# variant effect name array: full name\n",
    "print(\"Variant name array...\")\n",
    "oc_snpname_seg     = np.array(\n",
    "    [chrom+\":\"+x1+\" \"+x2 for x1,x2 in zip(\n",
    "        np.asarray(oc_genveff_seg_pos.astype(str)),\n",
    "        np.asarray([oc_genveff_seg_ann[i].split(\"|\")[10] for i,_ in enumerate(oc_genveff_seg_ann)]))\n",
    "    ])\n",
    "oc_snpname_seg = np.array([i.replace(\"p.\",\"\") for i in oc_snpname_seg])\n",
    "\n",
    "# variant effect name array: coding only\n",
    "oc_snpname_seg_cod = np.array(np.asarray([oc_genveff_seg_ann[i].split(\"|\")[10] for i,_ in enumerate(oc_genveff_seg_ann)]))\n",
    "oc_snpname_seg_cod = np.array([i.replace(\"p.\",\"\") for i in oc_snpname_seg_cod])\n",
    "# report\n",
    "print(oc_snpname_seg.shape,oc_snpname_seg_cod.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is variant above certain minimum frequency? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allele counts table per pop\n",
    "co_major = pd.DataFrame() \n",
    "co_minor = pd.DataFrame()\n",
    "fq_minor = pd.DataFrame()\n",
    "for popi in oc_popl: \n",
    "    co_major[popi] = oc_hapalco_pop_seg[popi][:,0]\n",
    "    co_minor[popi] = oc_hapalco_pop_seg[popi][:,1]\n",
    "    fq_minor[popi] = oc_hapalco_pop_seg[popi][:,1] / oc_hapalco_pop_seg[popi][:,0:2].sum(axis=1)\n",
    "co_major[\"all\"] = co_major.sum(axis=1)\n",
    "co_minor[\"all\"] = co_minor.sum(axis=1)\n",
    "fq_minor[\"all\"] = co_minor.sum(axis=1) / (co_minor.sum(axis=1)+co_major.sum(axis=1))\n",
    "\n",
    "# subset data: keep variants with at least 5% freq in at least one pop\n",
    "is_minfq  = np.logical_and(fq_minor.max(axis=1) > minfrq, fq_minor.min(axis=1) < 1-minfrq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is variant in the loci of interest? (useful for reporting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_in_loci = np.logical_and(oc_hapvars_seg[\"POS\"] >= loc_start, oc_hapvars_seg[\"POS\"] <= loc_end)\n",
    "is_coding  = np.asarray([oc_genveff_seg_ann[i].split(\"|\")[1] == \"missense_variant\" for i,_ in enumerate(oc_genveff_seg_ann)])\n",
    "is_report  = (is_in_loci & is_coding & is_minfq)\n",
    "\n",
    "# report\n",
    "print(\"minfrq, in loci and coding:\",sum(is_report))\n",
    "print(\"in loci:                   \",sum(is_in_loci))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print table with variants, effects and frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print table\n",
    "fq_repor = pd.DataFrame(data={\n",
    "    \"chr\"      : [chrom] * co_minor[is_in_loci].shape[0],\n",
    "    \"POS\"      : oc_hapvars_seg[\"POS\"].subset(sel0=is_in_loci)[:].astype(str),\n",
    "    \"REF\"      : oc_hapvars_seg[\"REF\"].subset(sel0=is_in_loci)[:].astype(str),\n",
    "    \"ALT\"      : oc_hapvars_seg[\"ALT\"].subset(sel0=is_in_loci)[:].astype(str),\n",
    "    \"is_nonsyn\": is_coding[is_in_loci],\n",
    "    \"gene_eff\" : [oc_genveff_seg_ann.compress(is_in_loci)[i].split(\"|\")[3]  for i,_ in enumerate(oc_genveff_seg_ann.compress(is_in_loci))],\n",
    "    \"PEP_eff\"  : [oc_genveff_seg_ann.compress(is_in_loci)[i].split(\"|\")[10] for i,_ in enumerate(oc_genveff_seg_ann.compress(is_in_loci))],\n",
    "    \"CDS_eff\"  : [oc_genveff_seg_ann.compress(is_in_loci)[i].split(\"|\")[9]  for i,_ in enumerate(oc_genveff_seg_ann.compress(is_in_loci))]\n",
    "},\n",
    "columns=[\"chr\",\"POS\",\"REF\",\"ALT\",\"gene_eff\",\"PEP_eff\",\"CDS_eff\",\"is_nonsyn\"]\n",
    ")\n",
    "fq_repor = pd.concat([fq_repor,fq_minor[is_in_loci].reset_index()],axis=1)\n",
    "fq_repor.to_csv(\"%s/%s_%s.allele_fq.csv\" % (outdir,outcode,l_nom),sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print table with genotypes of key variants, per sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with pops\n",
    "gt_repor = pd.DataFrame(data={\n",
    "    \"ox_code\"    : oc_samples[\"ox_code\"],\n",
    "    \"population\" : oc_samples[\"population\"],\n",
    "    \"species\"    : oc_samples[\"species\"],\n",
    "}\n",
    ")\n",
    "\n",
    "# retrieve 012 genotypes for report\n",
    "gt_gtysa = pd.DataFrame(np.transpose(oc_haploty_seg.subset(sel0=is_report).to_n_alt(fill=-1)))\n",
    "gt_gtysa.columns = oc_snpname_seg[is_report]\n",
    "\n",
    "# print\n",
    "gt_repor = pd.concat([gt_repor, gt_gtysa],axis=1)\n",
    "gt_repor.to_csv(\"%s/%s_%s.allele_genotypes.csv\" % (outdir,outcode,l_nom),sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add info from 2La karyotype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kary_df = pd.read_csv(kary_fn, sep='\\t')\n",
    "kary_df = kary_df.loc[kary_df['population'].isin(oc_popl)]\n",
    "print(\"karyotypes 2La phase2:\",kary_df.shape)\n",
    "\n",
    "kary_df_hap = pd.DataFrame(data={\n",
    "    \"ox_code\"     : list(itertools.chain(*[[ s + 'a', s + 'b'] for s in kary_df[\"ox_code\"].values.tolist()])),    \n",
    "    \"estimated_kt\" : list(itertools.chain(*[[ s      , s      ] for s in kary_df[\"estimated_kt\"].values.tolist()]))\n",
    "})\n",
    "\n",
    "kary_df_hap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant frequency\n",
    "\n",
    "First, plot frequency of coding variants inside of the gene itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minor allele freqs...\")\n",
    "    \n",
    "# plot minor allele freqs per pop\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "pdf = PdfPages(\"%s/%s_%s.allele_fq.pdf\" % (outdir,outcode,l_nom))\n",
    "\n",
    "# plot\n",
    "ax=sns.heatmap(fq_minor[is_report],vmin=0,vmax=0.5,cmap=sns.light_palette(\"darkslategray\",n_colors=31),\n",
    "               yticklabels=oc_snpname_seg[is_report],linewidths=0.8,linecolor=\"white\",annot=True)\n",
    "ax.set_title(\"ALT fq per pop %s\" % l_nom)\n",
    "\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variants seem to be linked (co-occurring in the same populations): 296S/345S, and 296G/345M. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardy-Weinberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alli_n,alli in enumerate([\"296S\",\"296G\"]):\n",
    "    for popi in [\"AOcol\",\"BFcol\",\"CIcol\",\"GHcol\",\"GNcol\",\"BFgam\",\"CMgam\",\"FRgam\",\"GAgam\",\"GHgam\",\"GNgam\",\"GQgam\",\"UGgam\",\"GM\",\"GW\",\"KE\",\"BFara\",\"CMara\",\"TZara\",\n",
    "         \"ZMqua\",\"SAmer\",\"KEmer\",\"CMmel\"]:\n",
    "\n",
    "        hw_obs = allel.heterozygosity_observed( g=oc_haploty_seg.subset(sel0=is_report, sel1=oc_popdict[popi]))\n",
    "        hw_exp = allel.heterozygosity_expected(af=oc_haploty_seg.subset(sel0=is_report, sel1=oc_popdict[popi]).count_alleles().to_frequencies(), ploidy=2)\n",
    "        hw_inb = allel.inbreeding_coefficient(  g=oc_haploty_seg.subset(sel0=is_report, sel1=oc_popdict[popi]))\n",
    "\n",
    "        hw_obs_n = hw_obs * len(oc_popdict[popi])\n",
    "        hw_exp_n = hw_exp * len(oc_popdict[popi])\n",
    "\n",
    "        print(alli,popi,int(hw_obs_n[alli_n]),hw_exp_n[alli_n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linkage disequilibrium\n",
    "\n",
    "We calculate the **linkage disequilibrium** between all allele pairs, using two different metrics (Rogers & Huff $r$, Lewontin's $D'$). \n",
    "\n",
    "Two functions for Lewontin $D'$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lewontin_d_prime(h, i, j, a=1, b=1):\n",
    "    h = allel.HaplotypeArray(h)\n",
    "    n_a = n_b = 0  # allele counts\n",
    "    n_ab = 0  # haplotype counts\n",
    "    n = 0  # allele number (i.e., number of calls)\n",
    "    for k in range(h.n_haplotypes): # iterate over haplotypes, counting alleles and haplotypes\n",
    "        allele_ik = h[i, k]\n",
    "        allele_jk = h[j, k]\n",
    "        if allele_ik < 0 or allele_jk < 0:    continue\n",
    "        if allele_ik == a:                    n_a += 1\n",
    "        if allele_jk == b:                    n_b += 1\n",
    "        if allele_ik == a and allele_jk == b: n_ab += 1\n",
    "        n += 1\n",
    "    if n == 0 or n_a == 0 or n_b == 0 or n == n_a or n == n_b:\n",
    "        return None # bail out if no data or either allele is absent or fixed\n",
    "    # compute coefficient of linkage disequilibrium * n**2\n",
    "    D_ab = (n * n_ab) - (n_a * n_b)\n",
    "    # compute normalisation coefficient * n**2\n",
    "    if D_ab >= 0: D_max = min(n_a * (n - n_b), (n - n_a) * n_b)\n",
    "    else:         D_max = min(n_a * n_b, (n - n_a) * (n - n_b))\n",
    "    # compute D prime\n",
    "    D_prime = D_ab / D_max\n",
    "    return D_prime\n",
    "\n",
    "def lewontin_d_prime_varloop(h):\n",
    "    n = len(h)\n",
    "    ld = np.zeros((n, n), dtype='f8')\n",
    "    for i,_ in enumerate(h):\n",
    "        for j,_ in enumerate(h):\n",
    "            if i != j:\n",
    "                ld[i, j] = lewontin_d_prime(h=h,i=i,j=j)\n",
    "    return(ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LD calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot minor allele freqs per pop\n",
    "pdf = PdfPages(\"%s/%s_%s.allele_ld.pdf\" % (outdir,outcode,l_nom))\n",
    "\n",
    "# linkage disequilibrium Rogers and Huff\n",
    "print(\"LD Rogers & Huff...\")\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "ld_rhr = allel.rogers_huff_r(oc_haploty_seg.compress(is_report).to_n_alt(fill=-1))\n",
    "ld_rhr = squareform(ld_rhr)\n",
    "np.fill_diagonal(ld_rhr,np.nan)\n",
    "ax=sns.heatmap(ld_rhr,vmin=-1,vmax=1,cmap=sns.diverging_palette(20,255,s=99,sep=15,l=45,n=31),\n",
    "               xticklabels=oc_snpname_seg[is_report],yticklabels=oc_snpname_seg[is_report],linewidths=0.8,linecolor=\"white\",annot=True)\n",
    "ax.set_title(\"Rogers & Huff $r$ %s\" % l_nom)\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "\n",
    "# print table\n",
    "ld_rhr_df = pd.DataFrame(ld_rhr)\n",
    "ld_rhr_df.columns = oc_snpname_seg[is_report]\n",
    "ld_rhr_df.rows    = oc_snpname_seg[is_report]\n",
    "ld_rhr_df.to_csv(\"%s/%s_%s.allele_ld_rhr.csv\" % (outdir,outcode,l_nom),sep=\"\\t\",index=False)\n",
    "\n",
    "# lewontin D' linkage disequilibrium\n",
    "print(\"LD Lewontin D'...\")\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "ld_lewdp = lewontin_d_prime_varloop(h=oc_haploty_seg.compress(is_report).to_n_alt(fill=-1))     \n",
    "np.fill_diagonal(ld_lewdp,np.nan)\n",
    "ax=sns.heatmap(ld_lewdp,vmin=-1,vmax=1,cmap=sns.diverging_palette(20,255,s=99,sep=15,l=45,n=31),\n",
    "               xticklabels=oc_snpname_seg[is_report],yticklabels=oc_snpname_seg[is_report],linewidths=0.8,linecolor=\"white\",annot=True)\n",
    "ax.set_title(\"Lewontin $D'$ %s\" % l_nom)\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "pdf.close()\n",
    "\n",
    "# print table\n",
    "ld_lewdp_df = pd.DataFrame(ld_lewdp)\n",
    "ld_lewdp_df.columns = oc_snpname_seg[is_report]\n",
    "ld_lewdp_df.rows    = oc_snpname_seg[is_report]\n",
    "ld_lewdp_df.to_csv(\"%s/%s_%s.allele_ld_lewdp.csv\" % (outdir,outcode,l_nom),sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat, per population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PdfPages(\"%s/%s_%s.allele_ld_perpop.pdf\" % (outdir,outcode,l_nom))\n",
    "\n",
    "for popi in oc_popl:\n",
    "    \n",
    "    h_popi = oc_haploty_seg.subset(sel0=is_report,sel1=oc_popdict[popi]).to_n_alt(fill=-1)[:]\n",
    "    #h_popi[h_popi > 1] = 1\n",
    "    \n",
    "    # r\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ld_rhr_i = allel.rogers_huff_r(h_popi)\n",
    "    ld_rhr_i = squareform(ld_rhr_i)\n",
    "    np.fill_diagonal(ld_rhr_i,np.nan)\n",
    "    ax=sns.heatmap(ld_rhr_i,vmin=-1,vmax=1,cmap=sns.diverging_palette(20,255,s=99,sep=15,l=45,n=31),\n",
    "               xticklabels=oc_snpname_seg[is_report],yticklabels=oc_snpname_seg[is_report],linewidths=0.8,linecolor=\"white\",annot=True)\n",
    "    ax.set_title(\"Rogers & Huff $r$ %s | population %s | n = %i\" % (l_nom,popi, h_popi.shape[1]))\n",
    "    \n",
    "    # D'\n",
    "    ax1 = plt.subplot(1, 2, 2)\n",
    "    ld_lewdp_i = lewontin_d_prime_varloop(h=h_popi)\n",
    "    np.fill_diagonal(ld_lewdp_i,np.nan)\n",
    "    ax=sns.heatmap(ld_lewdp_i,vmin=-1,vmax=1,cmap=sns.diverging_palette(20,255,s=99,sep=15,l=45,n=31),\n",
    "               xticklabels=oc_snpname_seg[is_report],linewidths=0.8,linecolor=\"white\",annot=True)\n",
    "    ax.set_title(\"Rogers & Huff $r$ %s | population %s | n = %i\" % (l_nom,popi, h_popi.shape[1]))\n",
    "    pdf.savefig(fig,bbox_inches='tight')\n",
    "    \n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat, but include all SNPs in Vgsc too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_in_vgsc     = np.logical_and(oc_hapvars_seg[\"POS\"] >= 2358158, oc_hapvars_seg[\"POS\"] <= 2431617)\n",
    "is_coding_vgsc = np.asarray([oc_genveff_seg_ann[i].split(\"|\")[1] == \"missense_variant\" for i,_ in enumerate(oc_genveff_seg_ann)])\n",
    "is_report_vgsc = (is_in_vgsc & is_coding_vgsc & is_minfq)\n",
    "is_report_vgsc_rdl = np.logical_or(is_report_vgsc, is_report)\n",
    "sum(is_report_vgsc_rdl)\n",
    "\n",
    "## ALTERNATIVE: only Rdl and 995FS alleles\n",
    "# is_995FS            = np.logical_or(oc_hapvars_seg[\"POS\"] == 2422651, oc_hapvars_seg[\"POS\"] == 2422652)  # genotype of 995S or 995F\n",
    "# is_report_with_vgsc = np.logical_or(is_995FS, is_report)\n",
    "# sum(is_report_with_vgsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot minor allele freqs per pop\n",
    "pdf = PdfPages(\"%s/%s_%s.allele_ld_vgsc.pdf\" % (outdir,outcode,l_nom))\n",
    "\n",
    "# linkage disequilibrium Rogers and Huff\n",
    "print(\"LD Rogers & Huff...\")\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ld_rhr = allel.rogers_huff_r(oc_haploty_seg.compress(is_report_vgsc_rdl).to_n_alt(fill=-1))\n",
    "ld_rhr = squareform(ld_rhr)\n",
    "np.fill_diagonal(ld_rhr,np.nan)\n",
    "ax=sns.heatmap(ld_rhr,vmin=-1,vmax=1,cmap=sns.diverging_palette(20,255,s=99,sep=15,l=45,n=31),\n",
    "               xticklabels=oc_snpname_seg[is_report_vgsc_rdl],yticklabels=oc_snpname_seg[is_report_vgsc_rdl],linewidths=0.8,linecolor=\"white\",annot=True)\n",
    "ax.set_title(\"Rogers & Huff $r$ %s\" % l_nom)\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "\n",
    "# print table\n",
    "ld_rhr_df = pd.DataFrame(ld_rhr)\n",
    "ld_rhr_df.columns = oc_snpname_seg[is_report_vgsc_rdl]\n",
    "ld_rhr_df.rows    = oc_snpname_seg[is_report_vgsc_rdl]\n",
    "ld_rhr_df.to_csv(\"%s/%s_%s.allele_ld_rhr_vgsc.csv\" % (outdir,outcode,l_nom),sep=\"\\t\",index=False)\n",
    "\n",
    "# lewontin D' linkage disequilibrium\n",
    "print(\"LD Lewontin D'...\")\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ld_lewdp = lewontin_d_prime_varloop(h=oc_haploty_seg.compress(is_report_vgsc_rdl).to_n_alt(fill=-1))     \n",
    "np.fill_diagonal(ld_lewdp,np.nan)\n",
    "ax=sns.heatmap(ld_lewdp,vmin=-1,vmax=1,cmap=sns.diverging_palette(20,255,s=99,sep=15,l=45,n=31),\n",
    "               xticklabels=oc_snpname_seg[is_report_vgsc_rdl],yticklabels=oc_snpname_seg[is_report_vgsc_rdl],linewidths=0.8,linecolor=\"white\",annot=True)\n",
    "ax.set_title(\"Lewontin $D'$ %s\" % l_nom)\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "pdf.close()\n",
    "\n",
    "# print table\n",
    "ld_lewdp_df = pd.DataFrame(ld_lewdp)\n",
    "ld_lewdp_df.columns = oc_snpname_seg[is_report_vgsc_rdl]\n",
    "ld_lewdp_df.rows    = oc_snpname_seg[is_report_vgsc_rdl]\n",
    "ld_lewdp_df.to_csv(\"%s/%s_%s.allele_ld_lewdp_vgsc.csv\" % (outdir,outcode,l_nom),sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linkage disequilibrium between all alleles (only $r$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_locifq = (is_in_loci & is_minfq)\n",
    "\n",
    "# linkage disequilibrium Rogers and Huff\n",
    "print(\"LD Rogers & Huff...\")\n",
    "ld_rhr_A = allel.rogers_huff_r(oc_haploty_seg.compress(is_locifq).to_n_alt(fill=-1))\n",
    "ld_rhr_A = squareform(ld_rhr_A)\n",
    "np.fill_diagonal(ld_rhr_A,np.nan)\n",
    "\n",
    "# print table\n",
    "ld_rhr_df = pd.DataFrame(ld_rhr_A)\n",
    "ld_rhr_df.columns = oc_snpname_seg[is_locifq]\n",
    "ld_rhr_df.rows    = oc_snpname_seg[is_locifq]\n",
    "ld_rhr_df.to_csv(\"%s/%s_%s.allele_ld_rhr_all.csv\" % (outdir,outcode,l_nom),sep=\"\\t\",index=False)\n",
    "\n",
    "print(\"Housekeeping...\")\n",
    "del ld_rhr_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haplotype networks\n",
    "\n",
    "Visualize haplotype similarity with **haplotype networks**, built from phased variants around the *Rdl* gene.\n",
    "\n",
    "We focus on the variants located around the 296th codon, +- 10kbp (`fbp_hap` variable) (i.e. core haplotype)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input hap networks\n",
    "loc_vari = 25429236  # variable to use as focus of hap networks -> position in the genome\n",
    "loc_varn = \"A296G\"   # name it\n",
    "\n",
    "# parameters hap networks\n",
    "fbp_hap  = 1e4     # num bp to retain around variant of interest (allele) to define core haplotyes, CC uses 6kbp\n",
    "max_dist = 3       # dist that breaks edges; default is 5; CC uses 2 -> if network method is MJN, then it doesn't work for max_dist>1!!!!\n",
    "max_alle = 1       # indirect variant breaks\n",
    "net_meth = \"msn\"   # can be: minimum_spanning_network msn ; minimum_spanning_tree mst ; median_joining_network mjt\n",
    "min_fq_h = 0.01    # min freq of the haplotype cluster, below that the component will be excluded from plots\n",
    "min_fc_h = int(min_fq_h*oc_haploty_hap_seg.shape[1])\n",
    "\n",
    "loc_vari_bool = np.any(oc_hapvars[\"POS\"]   == loc_vari)\n",
    "loc_vari_ix   = np.where(oc_hapvars[\"POS\"] == loc_vari)[0][0]\n",
    "\n",
    "# report\n",
    "print(\"Is %i %s in phased array? %s: index %i \" % (loc_vari,loc_varn,loc_vari_bool,loc_vari_ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset haplotypes to focus on variants around focal locus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset hap arrays to fit loci of interest (loci centre +/- fbp_hap)\n",
    "loc_pos_ix = allel.SortedIndex(oc_hapvars_seg['POS'])\n",
    "loc_pos_rn = loc_pos_ix.locate_range(loc_vari-fbp_hap,loc_vari+fbp_hap)\n",
    "\n",
    "# snp name\n",
    "oc_snpname_seg_mis = oc_snpname_seg_cod\n",
    "oc_snpname_seg_mis[np.invert(is_coding)] = \"\"\n",
    "loc_hap    = oc_haploty_hap_seg[loc_pos_rn]\n",
    "loc_snpnom = oc_snpname_seg_mis[loc_pos_rn]\n",
    "\n",
    "# report\n",
    "print(\"Phased variants and samples around %s:\" % loc_vari,loc_hap.shape)\n",
    "\n",
    "# color arrays\n",
    "loc_colpop = np.array([pop_colors[p] for p in oc_sampleh[\"population\"].values])\n",
    "loc_colpos = np.array([pos_colors[p] for p in oc_sampleh[\"population\"].values])\n",
    "loc_gty_A  = oc_haploty_hap_seg[ np.where(loc_pos_ix==loc_vari)[0][0] ]       # genotype of A296G\n",
    "loc_gty_B  = oc_haploty_hap_seg[ np.where(loc_pos_ix==25429235)[0][0] ] * 2   # genotype of A296S\n",
    "loc_gty_T  = loc_gty_A+loc_gty_B\n",
    "loc_colvar = np.array([var_colors[p] for p in loc_gty_T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build **hap networks colored by population**, and store which samples are in which component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_graph, loc_distinct_sets, loc_components = graph_haplotype_network(\n",
    "    h=loc_hap,\n",
    "    max_dist=max_dist,   \n",
    "    max_allele=max_alle, # for MJN only; default is 3\n",
    "    show_node_labels=min_fc_h,\n",
    "    distance_metric='hamming',network_method=net_meth,\n",
    "    hap_colors=loc_colpop,variant_labels=loc_snpnom,\n",
    "    return_components=True,show_singletons=False)\n",
    "\n",
    "num_components = loc_components.shape[0]\n",
    "\n",
    "# plot pdf\n",
    "loc_graph.format = 'pdf'\n",
    "loc_graph.attr(label='\\n\\n%s %s %s:%i\\n%i haps clustered into %i clusters with %s maxdist %i, from %i phased vars located +/- %i bp' % (l_nom,loc_varn,chrom,loc_vari,loc_hap.shape[1],num_components,net_meth,max_dist,loc_hap.shape[0],fbp_hap))\n",
    "fn = \"%s/%s_%s.hn_%s_var_%s_%s-pop\" % (outdir,outcode,l_nom,net_meth,loc_vari,loc_varn)\n",
    "loc_graph.render(fn,cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map hap clusters with REF alleles to actual lists of populations\n",
    "# BEWARE: identify_components fails if max_dist > 1 in MJN\n",
    "def identify_components(h_distinct_sets, components):\n",
    "    \"\"\"This function is designed to collect all indices for original haplotypes \n",
    "    within each connected component. I.e., it finds clusters from the network.\"\"\"\n",
    "    clusters = []\n",
    "    for c in np.unique(components):\n",
    "        cluster = set()\n",
    "        for i in np.nonzero(components == c)[0]:\n",
    "            cluster |= h_distinct_sets[i]\n",
    "        clusters.append(sorted(cluster))\n",
    "    return clusters\n",
    "\n",
    "loc_components_id      = identify_components(loc_distinct_sets, loc_components) \n",
    "loc_components_id_hapi = []\n",
    "loc_components_id_clui = []\n",
    "loc_components_id_dati = pd.DataFrame()\n",
    "for n,i in enumerate(loc_components_id):\n",
    "    loc_components_id_hapi = loc_components_id_hapi + loc_components_id[n]\n",
    "    loc_components_id_clui = loc_components_id_clui + list([n]*len(loc_components_id[n]))\n",
    "\n",
    "loc_components_id_dati[\"hap_index\"]   = loc_components_id_hapi\n",
    "loc_components_id_dati[\"hap_cluster\"] = loc_components_id_clui\n",
    "loc_components_id_dati                = loc_components_id_dati.sort_values(by=\"hap_index\")\n",
    "loc_components_id_dati                = loc_components_id_dati.set_index(keys=\"hap_index\")\n",
    "loc_components_id_dati[\"pop\"]         = oc_sampleh[\"population\"].values\n",
    "loc_components_id_dati[\"hap_id\"]      = oc_sampleh[\"ox_code\"].values\n",
    "\n",
    "# report\n",
    "print(\"total haps:\",loc_hap.shape[1])\n",
    "print(\"num haps per cluster (only clusters n>=%i):\" % min_fc_h)\n",
    "print(loc_components_id_dati.groupby((\"hap_cluster\")).size()[loc_components_id_dati.groupby((\"hap_cluster\")).size()>=min_fc_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_components_id_dati[\"Rdl_296\"] = loc_gty_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which clusters should we print? Those with frequency > 1% in the cohort (`min_fc_h` and `min_fq_h` variables, defined above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size = min_fc_h\n",
    "    \n",
    "# select clusters\n",
    "clu_list_ids     = np.unique(loc_components_id_dati[\"hap_cluster\"],return_counts=True)[0]\n",
    "clu_list_cou     = np.unique(loc_components_id_dati[\"hap_cluster\"],return_counts=True)[1]\n",
    "clu_list_ids_fil = clu_list_ids[clu_list_cou >= min_cluster_size]\n",
    "clu_list_cou_fil = clu_list_cou[clu_list_cou >= min_cluster_size]\n",
    "loc_printhap     = np.isin(loc_components_id_dati[\"hap_cluster\"].values, clu_list_ids_fil)\n",
    "\n",
    "print(loc_varn,loc_vari,\"print clusters:\",clu_list_ids_fil,\"\\t\\tsize of clusters: \",clu_list_cou_fil)\n",
    "print(loc_varn,loc_vari,\"print clusters total size:\",sum(loc_printhap),\"/\",len(loc_printhap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_graph, loc_distinct_sets, loc_components = graph_haplotype_network(\n",
    "    h=loc_hap.subset(sel1=loc_printhap),\n",
    "    max_dist=max_dist,   \n",
    "    max_allele=max_alle, # for MJN only; default is 3\n",
    "    show_node_labels=100,\n",
    "    distance_metric='hamming',network_method=net_meth,\n",
    "    hap_colors=loc_colpop[loc_printhap],variant_labels=loc_snpnom,\n",
    "    return_components=True,show_singletons=False)\n",
    "\n",
    "num_components = loc_components.shape[0]\n",
    "\n",
    "# plot pdf\n",
    "loc_graph.format = 'pdf'\n",
    "loc_graph.attr(label='\\n\\n%s %s %s:%i\\n%i haps clustered into %i clusters with %s maxdist %i, from %i phased vars located +/- %i bp' % (l_nom,loc_varn,chrom,loc_vari,loc_hap.shape[1],num_components,net_meth,max_dist,loc_hap.shape[0],fbp_hap))\n",
    "fn = \"%s/%s_%s.hn_%s_var_%s_%s-pop_minfq\" % (outdir,outcode,l_nom,net_meth,loc_vari,loc_varn)\n",
    "loc_graph.render(fn,cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot hap networks colored by species:\n",
    "\n",
    "* col is red\n",
    "* gam is green\n",
    "* gamcol hybrids are blue\n",
    "* arabiensis is orange\n",
    "* quadriannulatus is gray\n",
    "* merus is slategray\n",
    "* melas is steelish-blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_graph, loc_distinct_sets, loc_components = graph_haplotype_network(\n",
    "    h=loc_hap.subset(sel1=loc_printhap),\n",
    "    max_dist=max_dist,   \n",
    "    max_allele=max_alle, # for MJN only; default is 3\n",
    "    show_node_labels=100,\n",
    "    distance_metric='hamming',network_method=net_meth,\n",
    "    hap_colors=loc_colpos[loc_printhap],variant_labels=loc_snpnom,\n",
    "    return_components=True,show_singletons=False)\n",
    "\n",
    "num_components = loc_components.shape[0]\n",
    "\n",
    "# plot pdf\n",
    "loc_graph.format = 'pdf'\n",
    "loc_graph.attr(label='\\n\\n%s %s %s:%i\\n%i haps clustered into %i clusters with %s maxdist %i, from %i phased vars located +/- %i bp' % (l_nom,loc_varn,chrom,loc_vari,loc_hap.shape[1],num_components,net_meth,max_dist,loc_hap.shape[0],fbp_hap))\n",
    "fn = \"%s/%s_%s.hn_%s_var_%s_%s-sps_minfq\" % (outdir,outcode,l_nom,net_meth,loc_vari,loc_varn)\n",
    "loc_graph.render(fn,cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot haplotype networks colored by genotype in the 296th codon:\n",
    "\n",
    "* wt is gray (0)\n",
    "* 296G is light blue (1)\n",
    "* 296S is dark blue (2)\n",
    "* 296S+296G is violet (3)\n",
    "* missing genotype is orange (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_graph, loc_distinct_sets, loc_components = graph_haplotype_network(\n",
    "    h=loc_hap.subset(sel1=loc_printhap),\n",
    "    max_dist=max_dist,   \n",
    "    max_allele=max_alle, # for MJN only; default is 3\n",
    "    show_node_labels=100,\n",
    "    distance_metric='hamming',network_method=net_meth,\n",
    "    hap_colors=loc_colvar[loc_printhap],variant_labels=loc_snpnom,\n",
    "    return_components=True,show_singletons=False)\n",
    "\n",
    "num_components = loc_components.shape[0]\n",
    "\n",
    "# plot pdf\n",
    "loc_graph.format = 'pdf'\n",
    "loc_graph.attr(label='\\n\\n%s %s %s:%i\\n%i haps clustered into %i clusters with %s maxdist %i, from %i phased vars located +/- %i bp' % (l_nom,loc_varn,chrom,loc_vari,loc_hap.shape[1],num_components,net_meth,max_dist,loc_hap.shape[0],fbp_hap))\n",
    "fn = \"%s/%s_%s.hn_%s_var_%s_%s-gty_minfq\" % (outdir,outcode,l_nom,net_meth,loc_vari,loc_varn)\n",
    "loc_graph.render(fn,cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add genotypes for 345th codon:\n",
    "\n",
    "* wt is 0\n",
    "* 995S is 1\n",
    "* 995F is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gty_A  = oc_haploty_hap_seg[ np.where(loc_pos_ix==25433170)[0][0] ] * 2   # genotype of 345S\n",
    "loc_gty_B  = oc_haploty_hap_seg[ np.where(loc_pos_ix==25433171)[0][0] ]       # genotype of 345M\n",
    "loc_gty_T  = loc_gty_A+loc_gty_B\n",
    "loc_components_id_dati[\"Rdl_345\"] = loc_gty_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot haplotype networks colored by genotype in Vgsc 995th codon:\n",
    "\n",
    "* wt is gray (0)\n",
    "* 995S is light blue (1)\n",
    "* 995F is dark blue (2)\n",
    "* 995S+995F is violet (3)\n",
    "* missing genotype is orange (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_gty_A  = oc_haploty_hap_seg[ np.where(loc_pos_ix==2422651)[0][0] ]       # genotype of 995S\n",
    "loc_gty_B  = oc_haploty_hap_seg[ np.where(loc_pos_ix==2422652)[0][0] ] * 2   # genotype of 995F\n",
    "loc_gty_T  = loc_gty_A+loc_gty_B\n",
    "loc_components_id_dati[\"Vgsc_995\"] = loc_gty_T\n",
    "loc_colvar = np.array([var_colors[p] for p in loc_gty_T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_graph, loc_distinct_sets, loc_components = graph_haplotype_network(\n",
    "    h=loc_hap.subset(sel1=loc_printhap),\n",
    "    max_dist=max_dist,   \n",
    "    max_allele=max_alle, # for MJN only; default is 3\n",
    "    show_node_labels=100,\n",
    "    distance_metric='hamming',network_method=net_meth,\n",
    "    hap_colors=loc_colvar[loc_printhap],variant_labels=loc_snpnom,\n",
    "    return_components=True,show_singletons=False)\n",
    "\n",
    "num_components = loc_components.shape[0]\n",
    "\n",
    "# plot pdf\n",
    "loc_graph.format = 'pdf'\n",
    "loc_graph.attr(label='\\n\\n%s %s %s:%i\\n%i haps clustered into %i clusters with %s maxdist %i, from %i phased vars located +/- %i bp' % (l_nom,loc_varn,chrom,loc_vari,loc_hap.shape[1],num_components,net_meth,max_dist,loc_hap.shape[0],fbp_hap))\n",
    "fn = \"%s/%s_%s.hn_%s_var_%s_%s-gtyvgsc_minfq\" % (outdir,outcode,l_nom,net_meth,loc_vari,loc_varn)\n",
    "loc_graph.render(fn,cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend for population colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot color legend\n",
    "# WARNING: colors might be slightly different because matplotlib and graphviz parse them differently\n",
    "pdf = PdfPages(\"%s/%s_%s.hn_legend.pdf\" % (outdir,outcode,l_nom))\n",
    "fig,ax = plt.subplots(figsize=(1,1))\n",
    "ax.set_axis_off()\n",
    "custom_lines = [mpatches.Patch(facecolor=pop_colors[coli]) for coli in pop_colors.keys()]\n",
    "plt.legend(labels=pop_colors.keys(),handles=custom_lines)\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, output table with clusters, for posterity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_components_id_dati.to_csv(\"%s/%s_%s.hn_result.csv\" % (outdir,outcode,l_nom),sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you examine this table and the haplotype networks, you'll see that:\n",
    "\n",
    "* 296G genotypes are clustered in the component `4`, with 651 specimens from many different populations of gam and col\n",
    "* 296S genotypes are clustered in component `34`, with 94 specimens from BFcol\n",
    "* wt genotypes are grouped in many other components\n",
    "\n",
    "These component codes are important for downstream analyses (pop distribution, positive selection analyses, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of populations per haplotype\n",
    "\n",
    "Barplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_barplot(ax,color=\"k\", labformat = \"{:.2f}\"):\n",
    "    rects = ax.patches\n",
    "    for rect in rects:\n",
    "        x_value = rect.get_width()\n",
    "        y_value = rect.get_y() + rect.get_height() / 2\n",
    "        space   = 5\n",
    "        ha      = 'left'\n",
    "        label   = labformat.format(x_value) ## annotates bars with height labels, with 2 decimal points\n",
    "        plt.annotate(label, (x_value, y_value), xytext=(space, 0), textcoords=\"offset points\", va='center', ha=ha, color=color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PdfPages(\"%s/%s_%s.hn_popcomp_perhap.pdf\" % (outdir,outcode,l_nom))\n",
    "\n",
    "for i,clui in enumerate(clu_list_ids_fil):\n",
    "\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.8, hspace=None)\n",
    "    \n",
    "    # and now: some barplots with pop composition of each cluster\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    hap_labl = \"Pop composition, cluster \"+str(clui)+\" n=\"+str(clu_list_cou_fil[i])\n",
    "    hap_clui = loc_components_id_dati[\"hap_id\"][loc_components_id_dati[\"hap_cluster\"] == int(clui)]\n",
    "    hap_popi = loc_components_id_dati[\"pop\"][loc_components_id_dati[\"hap_cluster\"] == int(clui)]\n",
    "    pie_labels = [\"AOcol\",\"BFcol\",\"CIcol\",\"GHcol\",\"GNcol\",\"BFgam\",\"CMgam\",\"FRgam\",\"GAgam\",\"GHgam\",\"GNgam\",\"GQgam\",\"UGgam\",\"GM\",\"GW\",\"KE\",\"BFara\",\"CMara\",\"TZara\"]\n",
    "    pie_counts = [len(np.where(hap_popi==popi)[0]) for popi in pie_labels]\n",
    "    pie_colors = [pop_colors[popi] for popi in pie_labels]\n",
    "    plt.barh(width=pie_counts[::-1],y=pie_labels[::-1],color=pie_colors[::-1])\n",
    "    ax1.set_xlim(0,300)\n",
    "    annotate_barplot(ax=ax1, labformat=\"{:.0f}\")\n",
    "    plt.title(hap_labl)\n",
    "\n",
    "    # and next: some barplots with pop % of each cluster\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    hap_labl = \"% of haps from each pop in cluster\"\n",
    "    pie_coutot = oc_sampleh.groupby(\"population\").size()\n",
    "    pie_coutot = pie_coutot[pie_labels]\n",
    "    pie_fracti = pie_counts / pie_coutot * 100\n",
    "    plt.barh(width=pie_fracti[::-1],y=pie_labels[::-1],color=pie_colors[::-1])\n",
    "    annotate_barplot(ax=ax2, labformat=\"{:.2f}\")\n",
    "    ax2.set_xlabel(\"%\")\n",
    "    ax2.set_xlim(0,100)\n",
    "\n",
    "    pdf.savefig(fig,bbox_inches='tight')\n",
    "    \n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pie plots of main haplotypes per population:\n",
    "\n",
    "* `4` is 296G\n",
    "* `34` is 296S\n",
    "* some wt are with frequency >1% have their own code and color scheme\n",
    "* `other` are all other wt, with low frequency (<1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PdfPages(\"%s/%s_%s.hn_popcomp_perpop.pdf\" % (outdir,outcode,l_nom))\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "for popi_n,popi in enumerate(oc_popl):\n",
    "    \n",
    "    ax2 = plt.subplot(6, 6, popi_n+1)\n",
    "\n",
    "    hap_popi = loc_components_id_dati[\"hap_cluster\"][loc_components_id_dati[\"pop\"] == popi]\n",
    "    pie_coun = [sum(hap_popi == i) for i in clu_list_ids_fil]\n",
    "    pie_coub = np.append(pie_coun, len(hap_popi)-sum(pie_coun))\n",
    "    pie_labb = np.append(clu_list_ids_fil, \"other\")\n",
    "    pie_titl = popi+\" n=\"+str(len(hap_popi))\n",
    "    ax2.pie(pie_coub,labels=pie_labb, autopct=\"%1.1f%%\")\n",
    "    plt.title(pie_titl)\n",
    "\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pdf = PdfPages(\"%s/%s_%s.hn_popcomp_perpop.pdf\" % (outdir,outcode,l_nom))\n",
    "\n",
    "for popi in oc_popl:\n",
    "    \n",
    "    fig = plt.figure(figsize=(2,2))\n",
    "    \n",
    "    ax2 = plt.subplot(1, 1, 1)\n",
    "\n",
    "    hap_popi = loc_components_id_dati[\"hap_cluster\"][loc_components_id_dati[\"pop\"] == popi]\n",
    "    pie_coun = [sum(hap_popi == i) for i in clu_list_ids_fil]\n",
    "    pie_coub = np.append(pie_coun, len(hap_popi)-sum(pie_coun))\n",
    "    pie_labb = np.append(clu_list_ids_fil, \"other\")\n",
    "    pie_titl = \"Pop composition \"+popi+\" n=\"+str(len(hap_popi))\n",
    "    ax2.pie(pie_coub,labels=pie_labb, autopct=\"%1.1f%%\")\n",
    "    plt.title(pie_titl)\n",
    "    pdf.savefig(fig,bbox_inches='tight')\n",
    "    \n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection signals in clusters\n",
    "\n",
    "We want to see if the resistance haplotypes defined above have positive selection. \n",
    "\n",
    "In this analysis, we include clusters those with resistance alleles (cluster 4 is 296G, cluster 34 is 296S):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size = 50\n",
    "    \n",
    "# select clusters\n",
    "clu_list_ids     = np.unique(loc_components_id_dati[\"hap_cluster\"],return_counts=True)[0]\n",
    "clu_list_cou     = np.unique(loc_components_id_dati[\"hap_cluster\"],return_counts=True)[1]\n",
    "clu_list_ids_fil = clu_list_ids[clu_list_cou >= min_cluster_size]\n",
    "clu_list_cou_fil = clu_list_cou[clu_list_cou >= min_cluster_size]\n",
    "\n",
    "print(loc_varn,loc_vari,\"print EHH for clusters:\",clu_list_ids_fil,\"\\t\\tsize of clusters: \",clu_list_cou_fil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary of associations haplotype-cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hap dictionary: similar to pop dict, but for associations hap-cluster\n",
    "popdich_clu = dict()\n",
    "# populate dict of interest\n",
    "for clui in clu_list_ids_fil: \n",
    "    clu_key = \"cluster_\"+str(clui)\n",
    "    popdich_clu[clu_key] = [oc_sampleh[\"ox_code\"].values.tolist().index(i) for i in loc_components_id_dati[\"hap_id\"].values[loc_components_id_dati[\"hap_cluster\"]==clui]]\n",
    "# populate dict with all other samples\n",
    "popdich_clu[\"cluster_no\"] = []\n",
    "for clui in clu_list_ids[clu_list_cou < min_cluster_size]: \n",
    "    popdich_clu[\"cluster_no\"] = popdich_clu[\"cluster_no\"] + [oc_sampleh[\"ox_code\"].values.tolist().index(i) for i in loc_components_id_dati[\"hap_id\"].values[loc_components_id_dati[\"hap_cluster\"]==clui]]\n",
    "\n",
    "oc_hapalco_hap_clu_seg = oc_haploty_hap_seg.count_alleles_subpops(subpops=popdich_clu)\n",
    "oc_hapalco_hap_clu_seg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EHH decay\n",
    "\n",
    "Now calculate **EHH decay** on the region of interest, using phased variants around it (+/- a certain number of bases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors        = cm.rainbow(np.linspace(0, 1, len(clu_list_ids_fil)+3))\n",
    "ehh_above_thr = 0.95\n",
    "ehh_below_thr = 0.05\n",
    "flank_bp_EHH  = 200000\n",
    "\n",
    "# variants to retain\n",
    "clu_varbool_up = np.logical_and(oc_hapvars_seg[\"POS\"] >= loc_vari-flank_bp_EHH, oc_hapvars_seg[\"POS\"] < loc_vari)\n",
    "clu_varbool_do = np.logical_and(oc_hapvars_seg[\"POS\"] > loc_vari, oc_hapvars_seg[\"POS\"] <= loc_vari+flank_bp_EHH)\n",
    "clu_varbool    = np.logical_or(clu_varbool_up,clu_varbool_do)\n",
    "\n",
    "# samples to remove from analysis (EHH function can't handle missing -1 data)\n",
    "rmv_miss_ix   = np.unique(np.where(oc_haploty_hap_seg.subset(sel0=clu_varbool) == -1)[1]).tolist()\n",
    "rmv_miss_bool = np.invert(np.isin(range(0,oc_haploty_hap_seg.n_haplotypes),test_elements=rmv_miss_ix))\n",
    "\n",
    "# positions\n",
    "clu_ehh_pos = oc_hapvars_seg[\"POS\"].subset(sel0=clu_varbool)\n",
    "\n",
    "# plot\n",
    "pdf = PdfPages(\"%s/%s_%s.sel_EHHdecay.pdf\" % (outdir,outcode,l_nom))\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax3 = plt.subplot(2, 1, 1)\n",
    "\n",
    "for i,clui in enumerate(np.append(clu_list_ids_fil,\"no\")):\n",
    "\n",
    "    clu_key = \"cluster_\"+str(clui)\n",
    "    print(\"EHH %s\" % clu_key)\n",
    "\n",
    "    # which variants include in the cluster-wise analysis of selection?\n",
    "    clu_sambool = np.isin(range(0,oc_haploty_hap_seg.n_haplotypes),test_elements=popdich_clu[clu_key])\n",
    "    clu_sambool = np.logical_and(clu_sambool,rmv_miss_bool)\n",
    "    \n",
    "    # calculate actual EHH\n",
    "    clu_ehh_up_i = allel.ehh_decay(h=oc_haploty_hap_seg.subset(sel0=clu_varbool_up,sel1=clu_sambool))\n",
    "    clu_ehh_do_i = allel.ehh_decay(h=oc_haploty_hap_seg.subset(sel0=clu_varbool_do,sel1=clu_sambool))\n",
    "    clu_ehh_i    = np.concatenate((clu_ehh_up_i[::-1],clu_ehh_do_i))\n",
    "    clu_ehh_i_ar = np.trapz(clu_ehh_i)\n",
    "    ehh_above_start = clu_ehh_pos.compress(clu_ehh_i > ehh_above_thr)[0]\n",
    "    ehh_above_end   = clu_ehh_pos.compress(clu_ehh_i > ehh_above_thr)[-1]\n",
    "    ehh_below_start = clu_ehh_pos.compress(clu_ehh_i < ehh_below_thr)[0]\n",
    "    ehh_below_end   = clu_ehh_pos.compress(clu_ehh_i < ehh_below_thr)[-1]\n",
    "\n",
    "    # lab is data\n",
    "    clu_lab    = \"%s, n=%i, a=%.3f\\nEHH>%.2f: %i bp %i-%i\\nEHH<%.2f: %i bp %i-%i\" % (\n",
    "        clu_key, len(popdich_clu[clu_key]),clu_ehh_i_ar, \n",
    "        ehh_above_thr, ehh_above_end-ehh_above_start, ehh_above_start, ehh_above_end,\n",
    "        ehh_below_thr, ehh_below_end-ehh_below_start, ehh_below_start, ehh_below_end\n",
    "    )\n",
    "    \n",
    "    # plot EHH background & foreground\n",
    "    ax3.plot(clu_ehh_pos/1e6,clu_ehh_i,color=colors[i],label=clu_lab,mfc='none')\n",
    "\n",
    "sns.despine(ax=ax3,offset=10)\n",
    "ax3.set_title(\"EHH decay, %s:%i +/- %i, n=%s vars\" % (chrom,loc_vari,flank_bp_EHH,clu_ehh_pos.shape[0]))\n",
    "ax3.set_xlabel(\"Mb\")\n",
    "ax3.set_ylabel(\"EHH\")\n",
    "ax3.set_xlim(clu_ehh_pos[0]/1e6,clu_ehh_pos[-1]/1e6)\n",
    "ax3.set_ylim(0,1)\n",
    "plt.axhline(ehh_above_thr, color='lightgray',linestyle=\":\",label=str(ehh_above_thr))\n",
    "plt.axhline(ehh_below_thr, color='lightgray',linestyle=\":\",label=str(ehh_below_thr))\n",
    "plt.axvline(loc_start/1e6, color='red',linestyle=\":\",label=\"locus\")\n",
    "plt.axvline(loc_end/1e6, color='red',linestyle=\":\",label=\"\")\n",
    "plt.axvline(inv_start/1e6, color='orange',linestyle=\":\",label=\"inversion\")\n",
    "plt.axvline(inv_end/1e6, color='orange',linestyle=\":\",label=\"\")\n",
    "ax3.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "# plot transcripts\n",
    "ax4 = plt.subplot(4, 1, 4)\n",
    "sns.despine(ax=ax4,offset=10)\n",
    "ax4.axes.get_xaxis().set_ticks([])\n",
    "ax4.axes.get_xlabel() == \"\"\n",
    "locus_genel = plot_transcripts(\n",
    "    geneset=geneset,chrom=chrom,\n",
    "    start=clu_ehh_pos[0],stop=clu_ehh_pos[-1],\n",
    "    height=0.2,label_transcripts=True,ax=ax4,label_axis=False,\n",
    "    color=\"slategray\")\n",
    "\n",
    "# save\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same, with higher resolution to be able to see exons in *Rdl*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors        = cm.rainbow(np.linspace(0, 1, len(clu_list_ids_fil)+3))\n",
    "ehh_above_thr = 0.2\n",
    "ehh_below_thr = 0.1\n",
    "flank_bp_EHH  = 100000\n",
    "\n",
    "# variants to retain\n",
    "clu_varbool_up = np.logical_and(oc_hapvars_seg[\"POS\"] >= loc_vari-flank_bp_EHH, oc_hapvars_seg[\"POS\"] < loc_vari)\n",
    "clu_varbool_do = np.logical_and(oc_hapvars_seg[\"POS\"] > loc_vari, oc_hapvars_seg[\"POS\"] <= loc_vari+flank_bp_EHH)\n",
    "clu_varbool    = np.logical_or(clu_varbool_up,clu_varbool_do)\n",
    "\n",
    "# samples to remove from analysis (EHH function can't handle missing -1 data)\n",
    "rmv_miss_ix   = np.unique(np.where(oc_haploty_hap_seg.subset(sel0=clu_varbool) == -1)[1]).tolist()\n",
    "rmv_miss_bool = np.invert(np.isin(range(0,oc_haploty_hap_seg.n_haplotypes),test_elements=rmv_miss_ix))\n",
    "\n",
    "# positions\n",
    "clu_ehh_pos = oc_hapvars_seg[\"POS\"].subset(sel0=clu_varbool)\n",
    "\n",
    "# plot\n",
    "pdf = PdfPages(\"%s/%s_%s.sel_EHHdecay_zoom.pdf\" % (outdir,outcode,l_nom))\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax3 = plt.subplot(2, 1, 1)\n",
    "\n",
    "for i,clui in enumerate(np.append(clu_list_ids_fil,\"no\")):\n",
    "\n",
    "    clu_key = \"cluster_\"+str(clui)\n",
    "    print(\"EHH %s\" % clu_key)\n",
    "\n",
    "    # which variants include in the cluster-wise analysis of selection?\n",
    "    clu_sambool = np.isin(range(0,oc_haploty_hap_seg.n_haplotypes),test_elements=popdich_clu[clu_key])\n",
    "    clu_sambool = np.logical_and(clu_sambool,rmv_miss_bool)\n",
    "    \n",
    "    # calculate actual EHH\n",
    "    clu_ehh_up_i = allel.ehh_decay(h=oc_haploty_hap_seg.subset(sel0=clu_varbool_up,sel1=clu_sambool))\n",
    "    clu_ehh_do_i = allel.ehh_decay(h=oc_haploty_hap_seg.subset(sel0=clu_varbool_do,sel1=clu_sambool))\n",
    "    clu_ehh_i    = np.concatenate((clu_ehh_up_i[::-1],clu_ehh_do_i))\n",
    "    clu_ehh_i_ar = np.trapz(clu_ehh_i)\n",
    "    ehh_above_start = clu_ehh_pos.compress(clu_ehh_i > ehh_above_thr)[0]\n",
    "    ehh_above_end   = clu_ehh_pos.compress(clu_ehh_i > ehh_above_thr)[-1]\n",
    "    ehh_below_start = clu_ehh_pos.compress(clu_ehh_i < ehh_below_thr)[0]\n",
    "    ehh_below_end   = clu_ehh_pos.compress(clu_ehh_i < ehh_below_thr)[-1]\n",
    "\n",
    "    # lab is data\n",
    "    clu_lab    = \"%s, n=%i, a=%.3f\\nEHH>%.2f: %i bp %i-%i\\nEHH<%.2f: %i bp %i-%i\" % (\n",
    "        clu_key, len(popdich_clu[clu_key]),clu_ehh_i_ar, \n",
    "        ehh_above_thr, ehh_above_end-ehh_above_start, ehh_above_start, ehh_above_end,\n",
    "        ehh_below_thr, ehh_below_end-ehh_below_start, ehh_below_start, ehh_below_end\n",
    "    )\n",
    "    \n",
    "    # plot EHH background & foreground\n",
    "    ax3.plot(clu_ehh_pos/1e6,clu_ehh_i,color=colors[i],label=clu_lab,mfc='none')\n",
    "\n",
    "sns.despine(ax=ax3,offset=10)\n",
    "ax3.set_title(\"EHH decay, %s:%i +/- %i, n=%s vars\" % (chrom,loc_vari,flank_bp_EHH,clu_ehh_pos.shape[0]))\n",
    "ax3.set_xlabel(\"Mb\")\n",
    "ax3.set_ylabel(\"EHH\")\n",
    "ax3.set_xlim(clu_ehh_pos[0]/1e6,clu_ehh_pos[-1]/1e6)\n",
    "ax3.set_ylim(0,1)\n",
    "plt.axhline(ehh_above_thr, color='lightgray',linestyle=\":\",label=str(ehh_above_thr))\n",
    "plt.axhline(ehh_below_thr, color='lightgray',linestyle=\":\",label=str(ehh_below_thr))\n",
    "plt.axvline(loc_start/1e6, color='red',linestyle=\":\",label=\"locus\")\n",
    "plt.axvline(loc_end/1e6, color='red',linestyle=\":\",label=\"\")\n",
    "plt.axvline(inv_start/1e6, color='orange',linestyle=\":\",label=\"inversion\")\n",
    "plt.axvline(inv_end/1e6, color='orange',linestyle=\":\",label=\"\")\n",
    "ax3.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "# plot transcripts\n",
    "ax4 = plt.subplot(4, 1, 4)\n",
    "sns.despine(ax=ax4,offset=10)\n",
    "ax4.axes.get_xaxis().set_ticks([])\n",
    "ax4.axes.get_xlabel() == \"\"\n",
    "locus_genel = plot_transcripts(\n",
    "    geneset=geneset,chrom=chrom,\n",
    "    start=clu_ehh_pos[0],stop=clu_ehh_pos[-1],\n",
    "    height=0.2,label_transcripts=True,ax=ax4,label_axis=False,\n",
    "    color=\"slategray\")\n",
    "\n",
    "# save\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we'll plot a similar EHH plot but taking into account the karyotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_se_ci_report(x, ci=0.95, ddof=1):\n",
    "    s    = x[~np.isnan(x)]\n",
    "    n    = len(s)\n",
    "    av   = np.mean(s)\n",
    "    se   = np.std(s)\n",
    "    cili = (1 - ci) / 2.\n",
    "    cilo = np.quantile(np.sort(s), q=cili)\n",
    "    ciup = np.quantile(np.sort(s), q=(ci + cili))\n",
    "    return av, se, cilo, ciup, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors        = cm.rainbow(np.linspace(0, 1, len(clu_list_ids_fil)+3))\n",
    "ehh_above_thr = 0.95\n",
    "ehh_below_thr = 0.1\n",
    "flank_bp_EHH  = 150000\n",
    "\n",
    "# variants to retain\n",
    "clu_varbool_up = np.logical_and(oc_hapvars_seg[\"POS\"] >= loc_vari-flank_bp_EHH, oc_hapvars_seg[\"POS\"] < loc_vari)\n",
    "clu_varbool_do = np.logical_and(oc_hapvars_seg[\"POS\"] > loc_vari, oc_hapvars_seg[\"POS\"] <= loc_vari+flank_bp_EHH)\n",
    "clu_varbool    = np.logical_or(clu_varbool_up,clu_varbool_do)\n",
    "clu_varbool_genestart = np.logical_and(oc_hapvars_seg[\"POS\"] > loc_start-1e4, oc_hapvars_seg[\"POS\"] <= loc_start+1e4)\n",
    "clu_varbool_geneend   = np.logical_and(oc_hapvars_seg[\"POS\"] > loc_end-1e4, oc_hapvars_seg[\"POS\"] <= loc_end+1e4)\n",
    "clu_varbool_genecoreh = np.logical_and(oc_hapvars_seg[\"POS\"] > loc_vari-1e4, oc_hapvars_seg[\"POS\"] <= loc_vari+1e4)\n",
    "\n",
    "# samples to remove from analysis (EHH function can't handle missing -1 data)\n",
    "rmv_miss_ix   = np.unique(np.where(oc_haploty_hap_seg.subset(sel0=clu_varbool) == -1)[1]).tolist()\n",
    "rmv_miss_bool = np.invert(np.isin(range(0,oc_haploty_hap_seg.n_haplotypes),test_elements=rmv_miss_ix))\n",
    "\n",
    "# positions\n",
    "clu_ehh_pos = oc_hapvars_seg[\"POS\"].subset(sel0=clu_varbool)\n",
    "\n",
    "# plot\n",
    "pdf = PdfPages(\"%s/%s_%s.sel_EHHdecay_zoom_2Laaware.pdf\" % (outdir,outcode,l_nom))\n",
    "fig = plt.figure(figsize=(8,12))\n",
    "ax3 = plt.subplot(3, 1, 1)\n",
    "\n",
    "ax9 = plt.subplot(3, 1, 2)\n",
    "\n",
    "j=0\n",
    "for i,clui in enumerate([4]):\n",
    "    clu_key = \"cluster_\"+str(clui)\n",
    "    print(\"EHH %s\" % clu_key)\n",
    "\n",
    "    for _,ktj in enumerate([0,1,2]):\n",
    "        \n",
    "        # boolean for 2la karyotype\n",
    "        kt_bool  = kary_df_hap[\"estimated_kt\"] == ktj\n",
    "        kt_bool  = kt_bool.values\n",
    "        kt_bool  = np.isin(popdich_clu[clu_key], np.where(kt_bool)[0])\n",
    "        kt_index = np.array(popdich_clu[clu_key]).compress(kt_bool).tolist()\n",
    "\n",
    "        if sum(kt_bool)>1:\n",
    "            \n",
    "            # which variants include in the cluster-wise analysis of selection?\n",
    "            clu_sambool = np.isin(range(0,oc_haploty_hap_seg.n_haplotypes),test_elements=kt_index)\n",
    "            clu_sambool = np.logical_and(clu_sambool,rmv_miss_bool)\n",
    "\n",
    "            # calculate EHH\n",
    "            clu_ehh_up_i = allel.ehh_decay(h=oc_haploty_hap_seg.subset(sel0=clu_varbool_up,sel1=clu_sambool))\n",
    "            clu_ehh_do_i = allel.ehh_decay(h=oc_haploty_hap_seg.subset(sel0=clu_varbool_do,sel1=clu_sambool))\n",
    "            clu_ehh_i    = np.concatenate((clu_ehh_up_i[::-1],clu_ehh_do_i))\n",
    "            clu_ehh_i_ar = np.trapz(clu_ehh_i)\n",
    "            ehh_above_start = clu_ehh_pos.compress(clu_ehh_i > ehh_above_thr)[0]\n",
    "            ehh_above_end   = clu_ehh_pos.compress(clu_ehh_i > ehh_above_thr)[-1]\n",
    "            ehh_below_start = clu_ehh_pos.compress(clu_ehh_i > ehh_below_thr)[0]\n",
    "            ehh_below_end   = clu_ehh_pos.compress(clu_ehh_i > ehh_below_thr)[-1]\n",
    "            \n",
    "            # lab is data\n",
    "            clu_lab    = \"%s, n=%i, a=%.3f\\nEHH>%.2f: %i bp %i-%i\\nEHH>%.2f: %i bp %i-%i\" % (\n",
    "                clu_key+\"_kt_\"+str(ktj), len(kt_index),clu_ehh_i_ar, \n",
    "                ehh_above_thr, ehh_above_end-ehh_above_start, ehh_above_start, ehh_above_end,\n",
    "                ehh_below_thr, ehh_below_end-ehh_below_start, ehh_below_start, ehh_below_end\n",
    "            )\n",
    "\n",
    "            # plot EHH background & foreground\n",
    "            ax3.plot(clu_ehh_pos/1e6,clu_ehh_i,color=colors[j],label=clu_lab,mfc='none')\n",
    "            \n",
    "            # hap div\n",
    "            clu_pos_wib    = allel.stats.moving_statistic(oc_hapvars_seg[\"POS\"].subset(sel0=clu_varbool), statistic=lambda v: v[0], size=100, step=10)\n",
    "            clu_hdi_wib    = allel.moving_haplotype_diversity(oc_haploty_hap_seg.subset(sel0=clu_varbool,sel1=clu_sambool), size=100, step=10)\n",
    "            plt.subplot(3, 1, 2)\n",
    "            plt.step(clu_pos_wib/1e6, clu_hdi_wib, color=colors[j])\n",
    "\n",
    "            # haplotype diversity\n",
    "            j_run = len(kt_index)\n",
    "            j_hdi = np.zeros(shape=j_run)\n",
    "            for k in range(j_run):\n",
    "                j_sel1   = kt_index[0:k] + kt_index[k+1:j_run]\n",
    "                j_hdi[k] = allel.haplotype_diversity(oc_haploty_hap_seg.subset(sel0=clu_varbool_genestart, sel1=j_sel1))\n",
    "            j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_hdi)\n",
    "            print(\"Hap div gene start %s \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key+\"_kt_\"+str(ktj), j_av, j_se, j_cl, j_cu, j_nu))\n",
    "            \n",
    "            # haplotype diversity\n",
    "            j_run = len(kt_index)\n",
    "            j_hdi = np.zeros(shape=j_run)\n",
    "            for k in range(j_run):\n",
    "                j_sel1   = kt_index[0:k] + kt_index[k+1:j_run]\n",
    "                j_hdi[k] = allel.haplotype_diversity(oc_haploty_hap_seg.subset(sel0=clu_varbool_geneend, sel1=j_sel1))\n",
    "            j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_hdi)\n",
    "            print(\"Hap div gene end %s \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key+\"_kt_\"+str(ktj), j_av, j_se, j_cl, j_cu, j_nu))\n",
    "            \n",
    "            # haplotype diversity\n",
    "            j_run = len(kt_index)\n",
    "            j_hdi = np.zeros(shape=j_run)\n",
    "            for k in range(j_run):\n",
    "                j_sel1   = kt_index[0:k] + kt_index[k+1:j_run]\n",
    "                j_hdi[k] = allel.haplotype_diversity(oc_haploty_hap_seg.subset(sel0=clu_varbool_genecoreh, sel1=j_sel1))\n",
    "            j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_hdi)\n",
    "            print(\"Hap div gene corehap %s \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key+\"_kt_\"+str(ktj), j_av, j_se, j_cl, j_cu, j_nu))\n",
    "            \n",
    "            j=j+1\n",
    "\n",
    "sns.despine(ax=ax3,offset=10)\n",
    "ax3.set_title(\"EHH decay, %s:%i +/- %i, n=%s vars\" % (chrom,loc_vari,flank_bp_EHH,clu_ehh_pos.shape[0]))\n",
    "ax3.set_xlabel(\"Mb\")\n",
    "ax3.set_ylabel(\"EHH\")\n",
    "ax3.set_xlim(clu_ehh_pos[0]/1e6,clu_ehh_pos[-1]/1e6)\n",
    "ax3.set_ylim(0,1)\n",
    "plt.axhline(ehh_above_thr, color='lightgray',linestyle=\":\",label=str(ehh_above_thr))\n",
    "plt.axhline(ehh_below_thr, color='lightgray',linestyle=\":\",label=str(ehh_below_thr))\n",
    "plt.axvline(loc_start/1e6, color='red',linestyle=\":\",label=\"locus\")\n",
    "plt.axvline(loc_end/1e6, color='red',linestyle=\":\",label=\"\")\n",
    "plt.axvline(inv_start/1e6, color='orange',linestyle=\":\",label=\"inversion\")\n",
    "plt.axvline(inv_end/1e6, color='orange',linestyle=\":\",label=\"\")\n",
    "ax3.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "sns.despine(ax=ax9,offset=10)\n",
    "ax9.set_title(\"Hap diversity\")\n",
    "ax9.set_xlim(clu_ehh_pos[0]/1e6,clu_ehh_pos[-1]/1e6)\n",
    "ax9.set_ylim(0,1)\n",
    "ax9.set_xlabel(\"Mb\")\n",
    "ax9.set_ylabel(\"H\")\n",
    "\n",
    "# plot transcripts\n",
    "ax4 = plt.subplot(6, 1, 6)\n",
    "sns.despine(ax=ax4,offset=10)\n",
    "ax4.axes.get_xaxis().set_ticks([])\n",
    "ax4.axes.get_xlabel() == \"\"\n",
    "locus_genel = plot_transcripts(\n",
    "    geneset=geneset,chrom=chrom,\n",
    "    start=clu_ehh_pos[0],stop=clu_ehh_pos[-1],\n",
    "    height=0.2,label_transcripts=True,ax=ax4,label_axis=False,\n",
    "    color=\"slategray\")\n",
    "\n",
    "# save\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate sequence divergence (Dxy) between 296G in 0 or 2 backgrounds and wt in 0 or 2 backgrounds. We expect:\n",
    "\n",
    "* divergence between 296G and wt in the same background should be higher than divergence between 296G and wt from the opposite karyotype (ratio > 1 in the whole region)\n",
    "* however, divergence between 296G-2La (2) and wt-2L+a (0) will be LOWER THAN EXPECTED AT THE 3' end, due to the inter-karyotypic introgression of the swept haplotype (ratio < 1 at 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = cm.rainbow(np.linspace(0, 1, len(clu_list_ids_fil)+3))\n",
    "oc_hapalco_div = dict()\n",
    "clu_varbool = np.logical_and(oc_hapvars_seg[\"POS\"] > loc_start-1e6, oc_hapvars_seg[\"POS\"] <= loc_end+1e6)\n",
    "\n",
    "pdf = PdfPages(\"%s/%s_%s.sel_seqdivergence_zoom_2Laaware.pdf\" % (outdir,outcode,l_nom))\n",
    "\n",
    "for i,clui in enumerate([4]):\n",
    "    clu_key = \"cluster_\"+str(clui)\n",
    "\n",
    "    for _,ktj in enumerate([0,2]):\n",
    "        \n",
    "        # boolean for 2la karyotype\n",
    "        kt_bool  = kary_df_hap[\"estimated_kt\"] == ktj\n",
    "        kt_bool  = kt_bool.values\n",
    "        kt_bool  = np.isin(popdich_clu[clu_key], np.where(kt_bool)[0])\n",
    "        kt_index = np.array(popdich_clu[clu_key]).compress(kt_bool).tolist()\n",
    "\n",
    "        fig = plt.figure(figsize=(8,12))\n",
    "        ax3 = plt.subplot(3, 1, 1)\n",
    "        j=0\n",
    "       \n",
    "        for _,ktjnoc in enumerate([0,2]):\n",
    "            \n",
    "        \n",
    "            # boolean for 2la karyotype, for no cluster\n",
    "            kt_bool_noc  = kary_df_hap[\"estimated_kt\"] == ktjnoc\n",
    "            kt_bool_noc  = kt_bool_noc.values\n",
    "            kt_bool_noc  = np.isin(popdich_clu[\"cluster_no\"], np.where(kt_bool_noc)[0])\n",
    "            kt_index_noc = np.array(popdich_clu[\"cluster_no\"]).compress(kt_bool_noc).tolist()\n",
    "\n",
    "            if sum(kt_bool)>1:\n",
    "            \n",
    "                # which variants include in the cluster-wise analysis of selection?\n",
    "                clu_sambool = np.isin(range(0,oc_haploty_hap_seg.n_haplotypes),test_elements=kt_index)\n",
    "                # which variants include in the cluster-wise analysis of selection?\n",
    "                clu_sambool_noc = np.isin(range(0,oc_haploty_hap_seg.n_haplotypes),test_elements=kt_index_noc)\n",
    "\n",
    "                print(\"sim %s %i %i\" % (clu_key,ktj,sum(clu_sambool)))\n",
    "                print(\"sim %s %i %i\" % (clu_key,ktj,sum(clu_sambool_noc)))\n",
    "\n",
    "                oc_haploty_hap_seg_ktclu = oc_haploty_hap_seg.subset(sel0=clu_varbool,sel1=clu_sambool)\n",
    "                oc_haploty_hap_seg_ktnoc = oc_haploty_hap_seg.subset(sel0=clu_varbool,sel1=clu_sambool_noc)\n",
    "                oc_haploty_hap_seg_kt    = np.hstack((oc_haploty_hap_seg_ktclu,oc_haploty_hap_seg_ktnoc))\n",
    "                print(oc_haploty_hap_seg_ktclu.shape,oc_haploty_hap_seg_ktnoc.shape,oc_haploty_hap_seg_kt.shape)\n",
    "\n",
    "\n",
    "                # dict\n",
    "                oc_popdickar = dict()\n",
    "                oc_popdickar[\"clu\"] = list(range(0,oc_haploty_hap_seg_ktclu.shape[1]))\n",
    "                oc_popdickar[\"noc\"] = list(range(oc_haploty_hap_seg_ktclu.shape[1],oc_haploty_hap_seg_kt.shape[1]))\n",
    "\n",
    "                # allele counts\n",
    "                oc_hapalco_tmp = allel.HaplotypeChunkedArray(oc_haploty_hap_seg_kt).count_alleles_subpops(subpops=oc_popdickar)\n",
    "\n",
    "                # divergence clu to wt\n",
    "                label = clu_key+\" \"+str(ktj)+\" ~ wt \"+str(ktjnoc)\n",
    "                oc_hapalco_div[label] = allel.windowed_divergence(ac1=oc_hapalco_tmp[\"clu\"],ac2=oc_hapalco_tmp[\"noc\"],pos=oc_hapvars_seg[\"POS\"].subset(sel0=clu_varbool), size=20000,step=2000)\n",
    "\n",
    "                \n",
    "                plt.step(oc_hapalco_div[label][1][:,0]/1e6, oc_hapalco_div[label][0], color=colors[j], label=label)\n",
    "                sns.despine(ax=ax3,offset=10)\n",
    "                \n",
    "            \n",
    "            j=j+1\n",
    "        plt.axvline(loc_start/1e6, color='red',linestyle=\":\",label=\"locus\")\n",
    "        plt.axvline(loc_end/1e6, color='red',linestyle=\":\",label=\"\")\n",
    "        ax3.set_ylim(0,0.01)\n",
    "        ax3.set_xlim(clu_ehh_pos[0]/1e6,clu_ehh_pos[-1]/1e6)\n",
    "        ax3.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "        ax3.set_xlabel(\"Mb\")\n",
    "        ax3.set_ylabel(\"Dxy\")\n",
    "        ax3.set_title(\"Dxy 296G (%s) ~ wt (0 or 2)\" % ktj)\n",
    "        pdf.savefig(fig,bbox_inches='tight')\n",
    "\n",
    "\n",
    "# plot ratio\n",
    "fig = plt.figure(figsize=(8,12))\n",
    "ax3 = plt.subplot(3, 1, 1)\n",
    "sns.despine(ax=ax3,offset=10)\n",
    "plt.step(oc_hapalco_div[\"cluster_4 2 ~ wt 2\"][1][:,0]/1e6, \n",
    "         oc_hapalco_div[\"cluster_4 0 ~ wt 2\"][0] / oc_hapalco_div[\"cluster_4 0 ~ wt 0\"][0], \n",
    "         color=\"blue\",   label=\"cluster_4 0 ~ wt 2  /  cluster_4 0 ~ wt 0\")\n",
    "plt.step(oc_hapalco_div[\"cluster_4 2 ~ wt 2\"][1][:,0]/1e6, \n",
    "         oc_hapalco_div[\"cluster_4 2 ~ wt 2\"][0] / oc_hapalco_div[\"cluster_4 2 ~ wt 0\"][0], \n",
    "         color=\"m\", label=\"cluster_4 2 ~ wt 2  /  cluster_4 2 ~ wt 0\")\n",
    "plt.axhline(1, color='black',linestyle=\":\",label=\"\")\n",
    "plt.axvline(loc_start/1e6, color='red',linestyle=\":\",label=\"locus\")\n",
    "plt.axvline(loc_end/1e6, color='red',linestyle=\":\",label=\"\")\n",
    "ax3.set_xlim(clu_ehh_pos[0]/1e6,clu_ehh_pos[-1]/1e6)\n",
    "ax3.set_ylim(0,2.5)\n",
    "ax3.set_xlabel(\"Mb\")\n",
    "ax3.set_ylabel(\"Dxy ratio\")\n",
    "ax3.set_title(\"Dxy 296G~wt(diff kt) / 296G~wt(same kt)\")\n",
    "ax3.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "# save\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic differentiation in the core haplotype\n",
    "\n",
    "First, normalised hamming distance between haplotypes, in a pairwise fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corehap_varbool = np.logical_and(oc_hapvars_seg[\"POS\"] > loc_vari-fbp_hap, oc_hapvars_seg[\"POS\"] < loc_vari+fbp_hap)\n",
    "corehap_saminxs = np.append(np.append(popdich_clu[\"cluster_4\"], popdich_clu[\"cluster_34\"]),popdich_clu[\"cluster_no\"])\n",
    "#corehap_saminxs = np.append(popdich_clu[\"cluster_4\"], popdich_clu[\"cluster_34\"])\n",
    "corehap_genomat = oc_haploty_hap_seg.subset(\n",
    "    sel0=corehap_varbool, sel1=corehap_saminxs )\n",
    "corehap_genomat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PdfPages(\"%s/%s_%s.distmat.pdf\" % (outdir,outcode,l_nom))\n",
    "\n",
    "corehap_genodis = allel.pairwise_distance(\n",
    "    corehap_genomat[:],\n",
    "    metric=\"hamming\")\n",
    "arg_dic = {\"cmap\": \"GnBu\"}\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax2 = plt.subplot(1, 1, 1)\n",
    "allel.plot_pairwise_distance(np.sqrt(corehap_genodis),\n",
    "                                   colorbar=True, \n",
    "                                   imshow_kwargs=arg_dic,ax=ax2)\n",
    "\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, Fst between groups of haplotypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_len_snp = 5000\n",
    "step_frac_snp = 0.2\n",
    "step_perc_snp = int(step_frac_snp * 100)\n",
    "step_len_snp  = int(block_len_snp * step_frac_snp)\n",
    "\n",
    "def loop_Fst(name, popA_list, popC_list, \n",
    "             popA_ac, popC_ac,\n",
    "             pos,cycle = \"C\", block_len_snp=block_len_snp, step_len_snp=step_len_snp,\n",
    "             color=[\"blue\",\"darkorange\",\"turquoise\",\"crimson\",\"magenta\",\"limegreen\",\n",
    "                    \"forestgreen\",\"slategray\",\"orchid\",\"darkblue\"]):\n",
    "    \n",
    "    windows_pos = allel.stats.moving_statistic(pos, statistic=lambda v: v[0], size=block_len_snp,step=step_len_snp)\n",
    "\n",
    "    # calculate pvalues and focus in this region: duplicated region proper\n",
    "    is_locus = np.logical_and(pos > loc_start,pos < loc_end) # gene region\n",
    "    is_inv   = np.logical_and(pos > inv_start,pos < inv_end) # inversion region\n",
    "    \n",
    "    # loop\n",
    "    pdf = PdfPages(\"%s/%s.Fst_%s.pdf\" % (outdir,outcode,name))\n",
    "        \n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(popC_list)))\n",
    "\n",
    "    for an,popA in enumerate(popA_list):\n",
    "\n",
    "        fig = plt.figure(figsize=(15,3))\n",
    "\n",
    "        # whole chromosome: frame\n",
    "        ax1 = plt.subplot(1, 2, 1)\n",
    "        sns.despine(ax=ax1,offset=10)\n",
    "        ax1.set_title(\"Chr %s Fst %s~X\" % (chrom,popA))\n",
    "        ax1.set_xlim(0,50)\n",
    "        ax1.set_ylim(0,1)\n",
    "        ax1.set_xlabel(\"Mb\")\n",
    "        ax1.set_ylabel(\"Fst\")\n",
    "        plt.axhline(0, color='k',linestyle=\"--\",label=\"\")\n",
    "        plt.axvline(loc_start/1e6, color='red',linestyle=\":\",label=\"locus\")\n",
    "        plt.axvline(loc_end/1e6, color='red',linestyle=\":\",label=\"\")\n",
    "        plt.axvline(inv_start/1e6, color='orange',linestyle=\":\",label=\"inversion\")\n",
    "        plt.axvline(inv_end/1e6, color='orange',linestyle=\":\",label=\"\")\n",
    "\n",
    "        ax2 = plt.subplot(1, 4, 3)\n",
    "        sns.despine(ax=ax2,offset=10)\n",
    "        ax2.set_xlim(loc_start/1e6-0.5,loc_end/1e6+0.5)\n",
    "        ax2.set_ylim(0,1)\n",
    "        ax2.set_xlabel(\"Mb\")\n",
    "        ax2.set_ylabel(\"Fst\")\n",
    "        plt.axhline(0, color='k',linestyle=\"--\",label=\"\")\n",
    "        plt.axvline(loc_start/1e6, color='red',linestyle=\":\",label=\"locus\")\n",
    "        plt.axvline(loc_end/1e6, color='red',linestyle=\":\",label=\"\")\n",
    "        plt.axvline(inv_start/1e6, color='orange',linestyle=\":\",label=\"inversion\")\n",
    "        plt.axvline(inv_end/1e6, color='orange',linestyle=\":\",label=\"\")\n",
    "\n",
    "        for cn,popC in enumerate(popC_list):\n",
    "\n",
    "            if popA != popC:\n",
    "\n",
    "                # block-wise patterson D (normalised)\n",
    "                admix_pd_n_win = allel.moving_hudson_fst( \n",
    "                    ac1=popA_ac[popA][:,0:2],\n",
    "                    ac2=popC_ac[popC][:,0:2],\n",
    "                    size=block_len_snp,step=step_len_snp)\n",
    "\n",
    "                # whole chromosome: plot\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.step(windows_pos/1e6, admix_pd_n_win, color=colors[cn])\n",
    "\n",
    "                # estimated D in locus with pval\n",
    "                admix_pd_av_indup = allel.average_hudson_fst(\n",
    "                    ac1=popA_ac[popA][:,0:2][is_locus],\n",
    "                    ac2=popC_ac[popC][:,0:2][is_locus],\n",
    "                    blen=100)\n",
    "                # zoomed region: plot\n",
    "                plt.subplot(1, 4, 3)\n",
    "                plt.step(windows_pos/1e6, admix_pd_n_win, color=colors[cn], \n",
    "                         label=\"%s\\nFst = %.3f +/- %.3f\" % \n",
    "                             (popC,admix_pd_av_indup[0],admix_pd_av_indup[1]))\n",
    "\n",
    "        plt.axhline(0, color='k',linestyle=\"--\",label=\"\")\n",
    "        ax2.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "        # save pdf\n",
    "        pdf.savefig(fig,bbox_inches='tight')\n",
    "\n",
    "    pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "loop_Fst(\n",
    "    name=\"corehap\",\n",
    "    popA_list=[\"cluster_4\",\"cluster_34\",\"cluster_no\"],\n",
    "    popC_list=[\"cluster_4\",\"cluster_34\",\"cluster_no\"],\n",
    "    popA_ac=oc_hapalco_hap_clu_seg, \n",
    "    popC_ac=oc_hapalco_hap_clu_seg, \n",
    "    pos=oc_hapvars_seg[\"POS\"],\n",
    "    cycle=\"C\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence diversity\n",
    "\n",
    "Sequence diversity in each haplotype group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varbool_loc   = np.logical_and(oc_hapvars_seg[\"POS\"] >= loc_start, oc_hapvars_seg[\"POS\"] <= loc_end)\n",
    "varbool_syn   = np.asarray([oc_genveff_seg_ann[i].split(\"|\")[1] == \"synonymous_variant\" for i,_ in enumerate(oc_genveff_seg_ann)])\n",
    "varbool_nosyn = is_coding\n",
    "\n",
    "for popi in [\"cluster_34\",\"cluster_4\",\"cluster_no\"]:\n",
    "\n",
    "    j_run    = len(popdich_clu[popi])\n",
    "    j_pi_s   = np.zeros(shape=j_run)\n",
    "    j_pi_n   = np.zeros(shape=j_run)\n",
    "    j_pi_n_s = np.zeros(shape=j_run)\n",
    "    for i in range(j_run):\n",
    "        j_sel1      = popdich_clu[popi][0:i] + popdich_clu[popi][i+1:j_run]\n",
    "        j_dic       = dict()\n",
    "        j_dic[popi] = j_sel1\n",
    "        j_dic_n_ac  = oc_haploty_hap_seg.subset(sel0=np.logical_and(varbool_loc, varbool_nosyn)).count_alleles_subpops(subpops=j_dic)\n",
    "        j_dic_s_ac  = oc_haploty_hap_seg.subset(sel0=np.logical_and(varbool_loc, varbool_syn)).count_alleles_subpops(subpops=j_dic)\n",
    "        j_pi_n[i]   = allel.sequence_diversity(pos=oc_hapvars_seg[\"POS\"].subset(sel0=np.logical_and(varbool_loc, varbool_nosyn)), ac=j_dic_n_ac[popi])\n",
    "        j_pi_s[i]   = allel.sequence_diversity(pos=oc_hapvars_seg[\"POS\"].subset(sel0=np.logical_and(varbool_loc, varbool_syn)),   ac=j_dic_s_ac[popi])\n",
    "        j_pi_n_s[i] = j_pi_n[i] / j_pi_s[i]\n",
    "\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_pi_n)\n",
    "    print(\"pi_n %s \\t = %.3E +/- %.3E SE, %.3E-%.3E CI95, n=%i\" % (popi, j_av, j_se, j_cl, j_cu, j_nu))\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_pi_s)\n",
    "    print(\"pi_s %s \\t = %.3E +/- %.3E SE, %.3E-%.3E CI95, n=%i\" % (popi, j_av, j_se, j_cl, j_cu, j_nu))\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_pi_n_s)\n",
    "    print(\"r_n_s %s\\t = %.3E +/- %.3E SE, %.3E-%.3E CI95, n=%i\" % (popi, j_av, j_se, j_cl, j_cu, j_nu))\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garud H\n",
    "\n",
    "Compute **Garud H** for each cluster, along the entire chromosome, to see landscape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "block_len_hap = 500   # for the whole-genome view\n",
    "step_len_hap  = 100   # for the whole-genome view\n",
    "colors        = cm.rainbow(np.linspace(0, 1, len(clu_list_ids_fil)+3))\n",
    "fbp_hap_extra = 1e6\n",
    "\n",
    "# variants to retain\n",
    "print(\"Retain...\")\n",
    "clu_pos_wib    = allel.stats.moving_statistic(oc_hapvars_seg[\"POS\"], statistic=lambda v: v[0], size=block_len_hap,step=step_len_hap)\n",
    "\n",
    "# open pdf\n",
    "pdf = PdfPages(\"%s/%s_%s.sel_garudH.pdf\" % (outdir,outcode,l_nom))\n",
    "fig = plt.figure(figsize=(8,12))\n",
    "\n",
    "# whole chromosome: frame\n",
    "ax1 = plt.subplot(3, 1, 1)\n",
    "sns.despine(ax=ax1,offset=10)\n",
    "ax1.set_title(\"Garud H12\")\n",
    "ax1.set_xlim(ret_start/1e6,50)\n",
    "ax1.set_ylim(0,1)\n",
    "ax1.set_xlabel(\"Mb\")\n",
    "ax1.set_ylabel(\"H12\")\n",
    "\n",
    "ax2 = plt.subplot(3, 1, 3)\n",
    "sns.despine(ax=ax2,offset=10)\n",
    "ax2.set_title(\"Garud H2/H1\")\n",
    "ax2.set_xlim(ret_start/1e6,50)\n",
    "ax2.set_ylim(0,1)\n",
    "ax2.set_xlabel(\"Mb\")\n",
    "ax2.set_ylabel(\"H2/H1\")\n",
    "\n",
    "ax3 = plt.subplot(3, 1, 2)\n",
    "sns.despine(ax=ax3,offset=10)\n",
    "ax3.set_title(\"Hap diversity\")\n",
    "ax3.set_xlim(ret_start/1e6,50)\n",
    "ax3.set_ylim(0,1)\n",
    "ax3.set_xlabel(\"Mb\")\n",
    "ax3.set_ylabel(\"H\")\n",
    "\n",
    "for i,clui in enumerate(np.append(clu_list_ids_fil,\"no\")):\n",
    "    \n",
    "    # which variants include in the cluster-wise analysis of selection?\n",
    "    clu_key     = \"cluster_\"+str(clui)\n",
    "    clu_sambool = np.isin(range(0,oc_haploty_hap_seg.n_haplotypes),test_elements=popdich_clu[clu_key])\n",
    "        \n",
    "    # selection: Garud's H\n",
    "    print(\"Garud H %s - chr...\" % clu_key)\n",
    "    clu_gah_wib = allel.moving_garud_h(oc_haploty_hap_seg.subset(sel1=popdich_clu[clu_key]), size=block_len_hap, step=step_len_hap)\n",
    "\n",
    "    # haplotype diversity\n",
    "    print(\"Hap div %s - chr...\" % clu_key)\n",
    "    clu_hdi_wib = allel.moving_haplotype_diversity(oc_haploty_hap_seg.subset(sel1=popdich_clu[clu_key]), size=block_len_hap, step=step_len_hap)\n",
    "    \n",
    "    print(\"Plots   %s...\" % clu_key)\n",
    "    # plot H12\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.step(clu_pos_wib/1e6, clu_gah_wib[1], color=colors[i], label=clu_key)\n",
    "        \n",
    "    # plot H2H1\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.step(clu_pos_wib/1e6, clu_gah_wib[3], color=colors[i])\n",
    "\n",
    "    # plot hap div\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.step(clu_pos_wib/1e6, clu_hdi_wib, color=colors[i])\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.axvline(loc_start/1e6, color='red',linestyle=\":\",label=\"locus\")\n",
    "plt.axvline(loc_end/1e6, color='red',linestyle=\":\",label=\"\")\n",
    "plt.axvline(inv_start/1e6, color='orange',linestyle=\":\",label=\"inversion\")\n",
    "plt.axvline(inv_end/1e6, color='orange',linestyle=\":\",label=\"\")\n",
    "ax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "plt.axvline(2358158, color='brown',linestyle=\":\",label=\"Vgsc\")\n",
    "plt.axvline(2431617, color='brown',linestyle=\":\",label=\"\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.axvline(loc_start/1e6, color='red',linestyle=\":\",label=\"locus\")\n",
    "plt.axvline(loc_end/1e6, color='red',linestyle=\":\",label=\"\")\n",
    "plt.axvline(inv_start/1e6, color='orange',linestyle=\":\",label=\"inversion\")\n",
    "plt.axvline(inv_end/1e6, color='orange',linestyle=\":\",label=\"\")\n",
    "plt.axvline(2358158, color='brown',linestyle=\":\",label=\"Vgsc\")\n",
    "plt.axvline(2431617, color='brown',linestyle=\":\",label=\"\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.axvline(loc_start/1e6, color='red',linestyle=\":\",label=\"locus\")\n",
    "plt.axvline(loc_end/1e6, color='red',linestyle=\":\",label=\"\")\n",
    "plt.axvline(inv_start/1e6, color='orange',linestyle=\":\",label=\"inversion\")\n",
    "plt.axvline(inv_end/1e6, color='orange',linestyle=\":\",label=\"\")\n",
    "plt.axvline(2358158, color='brown',linestyle=\":\",label=\"Vgsc\")\n",
    "plt.axvline(2431617, color='brown',linestyle=\":\",label=\"\")\n",
    "\n",
    "pdf.savefig(fig,bbox_inches='tight')\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate selection statistics in the loci of interest\n",
    "\n",
    "Zoom into Rdl locus or core haplotype and recalculate local Garud H and haplotype diversity, with jack-knifing of samples.\n",
    "\n",
    "* **jack-knifing**: remove one sample (specimen) at a time without replacement and recalculate the statistic $n$ times ($n$=number of samples in each group, which will vary according to each haplotype cluster). This is the appropiate procedure for haplotype similarity statistics: a priori, the iterative removal of samples can both increase or decrease the statistic, which makes it a good measure of the robustness of the estimate.\n",
    "* **bootstraping**: resample specimens with replacement, for $n=100$ iterations (where the number of samples in each iteration equals the number of samples in each group). This is **not adequate** for this particular statistic, because when you resample with replacement you're introducing duplicated samples that will always result in an upwards bias for statistics that measure haplotype similarity.\n",
    "\n",
    "Selection statistics within core haplotype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu_varbool    = np.logical_and(oc_hapvars_seg[\"POS\"] >= loc_vari-fbp_hap , oc_hapvars_seg[\"POS\"] <= loc_vari+fbp_hap)\n",
    "\n",
    "for i,clui in enumerate(np.append(clu_list_ids_fil,\"no\")):\n",
    "\n",
    "    # cluster key\n",
    "    clu_key     = \"cluster_\"+str(clui)\n",
    "    print(\"Locus: %i vars, cluster %s\" % (sum(clu_varbool),clu_key))\n",
    "    \n",
    "    # haplotype diversity\n",
    "    j_run = len(popdich_clu[clu_key])\n",
    "    j_hdi = np.zeros(shape=j_run)\n",
    "    for i in range(j_run):\n",
    "        j_sel1   = popdich_clu[clu_key][0:i] + popdich_clu[clu_key][i+1:j_run]\n",
    "        j_hdi[i] = allel.haplotype_diversity(oc_haploty_hap_seg.subset(sel0=clu_varbool, sel1=j_sel1))\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_hdi)\n",
    "    print(\"Hap div %s \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key, j_av, j_se, j_cl, j_cu, j_nu))\n",
    "\n",
    "    # Garud H12\n",
    "    j_run  = len(popdich_clu[clu_key])\n",
    "    j_h12  = np.zeros(shape=j_run)\n",
    "    j_h2h1 = np.zeros(shape=j_run)\n",
    "    for i in range(j_run):\n",
    "        j_sel1    = popdich_clu[clu_key][0:i] + popdich_clu[clu_key][i+1:j_run]\n",
    "        j_hstat   = allel.garud_h(oc_haploty_hap_seg.subset(sel0=clu_varbool, sel1=j_sel1))\n",
    "        j_h12[i]  = j_hstat[1]\n",
    "        j_h2h1[i] = j_hstat[3]\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_h12)\n",
    "    print(\"H12 %s     \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key, j_av, j_se, j_cl, j_cu, j_nu))\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_h2h1)\n",
    "    print(\"H2H1 %s    \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key, j_av, j_se, j_cl, j_cu, j_nu))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, selection statistics within the gene of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu_varbool    = np.logical_and(oc_hapvars_seg[\"POS\"] >= loc_start , oc_hapvars_seg[\"POS\"] <= loc_end)\n",
    "\n",
    "for i,clui in enumerate(np.append(clu_list_ids_fil,\"no\")):\n",
    "\n",
    "    # cluster key\n",
    "    clu_key     = \"cluster_\"+str(clui)\n",
    "    print(\"Locus: %i vars, cluster %s\" % (sum(clu_varbool),clu_key))\n",
    "    \n",
    "    # haplotype diversity\n",
    "    j_run = len(popdich_clu[clu_key])\n",
    "    j_hdi = np.zeros(shape=j_run)\n",
    "    for i in range(j_run):\n",
    "        j_sel1   = popdich_clu[clu_key][0:i] + popdich_clu[clu_key][i+1:j_run]\n",
    "        j_hdi[i] = allel.haplotype_diversity(oc_haploty_hap_seg.subset(sel0=clu_varbool, sel1=j_sel1))\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_hdi)\n",
    "    print(\"Hap div %s \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key, j_av, j_se, j_cl, j_cu, j_nu))\n",
    "\n",
    "    # Garud H12\n",
    "    j_run  = len(popdich_clu[clu_key])\n",
    "    j_h12  = np.zeros(shape=j_run)\n",
    "    j_h2h1 = np.zeros(shape=j_run)\n",
    "    for i in range(j_run):\n",
    "        j_sel1    = popdich_clu[clu_key][0:i] + popdich_clu[clu_key][i+1:j_run]\n",
    "        j_hstat   = allel.garud_h(oc_haploty_hap_seg.subset(sel0=clu_varbool, sel1=j_sel1))\n",
    "        j_h12[i]  = j_hstat[1]\n",
    "        j_h2h1[i] = j_hstat[3]\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_h12)\n",
    "    print(\"H12 %s     \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key, j_av, j_se, j_cl, j_cu, j_nu))\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_h2h1)\n",
    "    print(\"H2H1 %s    \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key, j_av, j_se, j_cl, j_cu, j_nu))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, positive selection statistics in Vgsc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu_varbool    = np.logical_and(oc_hapvars_seg[\"POS\"] >= 2358158 , oc_hapvars_seg[\"POS\"] <= 2431617)\n",
    "\n",
    "for i,clui in enumerate(np.append(clu_list_ids_fil,\"no\")):\n",
    "\n",
    "    # cluster key\n",
    "    clu_key     = \"cluster_\"+str(clui)\n",
    "    print(\"Locus: %i vars, cluster %s\" % (sum(clu_varbool),clu_key))\n",
    "    \n",
    "    # haplotype diversity\n",
    "    j_run = len(popdich_clu[clu_key])\n",
    "    j_hdi = np.zeros(shape=j_run)\n",
    "    for i in range(j_run):\n",
    "        j_sel1   = popdich_clu[clu_key][0:i] + popdich_clu[clu_key][i+1:j_run]\n",
    "        j_hdi[i] = allel.haplotype_diversity(oc_haploty_hap_seg.subset(sel0=clu_varbool, sel1=j_sel1))\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_hdi)\n",
    "    print(\"Hap div %s \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key, j_av, j_se, j_cl, j_cu, j_nu))\n",
    "\n",
    "    # Garud H12\n",
    "    j_run  = len(popdich_clu[clu_key])\n",
    "    j_h12  = np.zeros(shape=j_run)\n",
    "    j_h2h1 = np.zeros(shape=j_run)\n",
    "    for i in range(j_run):\n",
    "        j_sel1    = popdich_clu[clu_key][0:i] + popdich_clu[clu_key][i+1:j_run]\n",
    "        j_hstat   = allel.garud_h(oc_haploty_hap_seg.subset(sel0=clu_varbool, sel1=j_sel1))\n",
    "        j_h12[i]  = j_hstat[1]\n",
    "        j_h2h1[i] = j_hstat[3]\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_h12)\n",
    "    print(\"H12 %s     \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key, j_av, j_se, j_cl, j_cu, j_nu))\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_h2h1)\n",
    "    print(\"H2H1 %s    \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key, j_av, j_se, j_cl, j_cu, j_nu))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing, around Vgsc's 995th codon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu_varbool    = np.logical_and(oc_hapvars_seg[\"POS\"] >= 2422651-6000 , oc_hapvars_seg[\"POS\"] <= 2422651+6000) # lengths from Clarkson et al biorxiv 2018\n",
    "\n",
    "for i,clui in enumerate(np.append(clu_list_ids_fil,\"no\")):\n",
    "\n",
    "    # cluster key\n",
    "    clu_key     = \"cluster_\"+str(clui)\n",
    "    print(\"Locus: %i vars, cluster %s\" % (sum(clu_varbool),clu_key))\n",
    "    \n",
    "    # haplotype diversity\n",
    "    j_run = len(popdich_clu[clu_key])\n",
    "    j_hdi = np.zeros(shape=j_run)\n",
    "    for i in range(j_run):\n",
    "        j_sel1   = popdich_clu[clu_key][0:i] + popdich_clu[clu_key][i+1:j_run]\n",
    "        j_hdi[i] = allel.haplotype_diversity(oc_haploty_hap_seg.subset(sel0=clu_varbool, sel1=j_sel1))\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_hdi)\n",
    "    print(\"Hap div %s \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key, j_av, j_se, j_cl, j_cu, j_nu))\n",
    "\n",
    "    # Garud H12\n",
    "    j_run  = len(popdich_clu[clu_key])\n",
    "    j_h12  = np.zeros(shape=j_run)\n",
    "    j_h2h1 = np.zeros(shape=j_run)\n",
    "    for i in range(j_run):\n",
    "        j_sel1    = popdich_clu[clu_key][0:i] + popdich_clu[clu_key][i+1:j_run]\n",
    "        j_hstat   = allel.garud_h(oc_haploty_hap_seg.subset(sel0=clu_varbool, sel1=j_sel1))\n",
    "        j_h12[i]  = j_hstat[1]\n",
    "        j_h2h1[i] = j_hstat[3]\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_h12)\n",
    "    print(\"H12 %s     \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key, j_av, j_se, j_cl, j_cu, j_nu))\n",
    "    j_av,j_se,j_cl,j_cu,j_nu = mean_se_ci_report(j_h2h1)\n",
    "    print(\"H2H1 %s    \\t = %.6f +/- %.6f SE, %.6f-%.6f CI95, n=%i\" % (clu_key, j_av, j_se, j_cl, j_cu, j_nu))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export FASTA\n",
    "\n",
    "Prepare sequence names. First, genotype in 296th codon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_sampleh[\"296_SG_genotype\"] = loc_gty_T.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add info from 2La karyotype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kary_df = pd.read_csv(kary_fn, sep='\\t')\n",
    "kary_df = kary_df.loc[kary_df['population'].isin(oc_popl)]\n",
    "print(\"karyotypes 2La phase2:\",kary_df.shape)\n",
    "\n",
    "kary_df_hap = pd.DataFrame(data={\n",
    "    \"ox_code\"     : list(itertools.chain(*[[ s + 'a', s + 'b'] for s in kary_df[\"ox_code\"].values.tolist()])),    \n",
    "    \"estimated_kt\" : list(itertools.chain(*[[ s      , s      ] for s in kary_df[\"estimated_kt\"].values.tolist()]))\n",
    "})\n",
    "\n",
    "kary_df_hap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create alignment dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happhy = pd.DataFrame({\n",
    "    \"hap\": \">\"+oc_sampleh[\"ox_code\"]+\"_\"+oc_sampleh[\"population\"]+\"_gt\"+oc_sampleh[\"296_SG_genotype\"]+\"_kt\"+kary_df_hap[\"estimated_kt\"].astype(str),\n",
    "    \"seq\": np.nan},    \n",
    "    columns=[\"hap\", \"seq\"])\n",
    "happhy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FASTA *Rdl*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_name        = \"loc\" \n",
    "loc_varbool        = np.logical_and(oc_hapvars_seg[\"POS\"][:] >= loc_start, oc_hapvars_seg[\"POS\"][:] <= loc_end)\n",
    "fa_haploty_hap_seg = oc_haploty_hap_seg.compress(loc_varbool)\n",
    "fa_hapvars_seg     = oc_hapvars_seg.compress(loc_varbool)\n",
    "print(export_name,fa_haploty_hap_seg.shape, fa_hapvars_seg.shape)\n",
    "\n",
    "for pn,popi in enumerate(oc_sampleh[\"ox_code\"]):\n",
    "    \n",
    "    if pn % int(happhy.shape[0]/10) == 0 : print(pn,\"/\",happhy.shape[0])\n",
    "    popi_gen = np.ndarray.tolist(fa_haploty_hap_seg[:,pn])\n",
    "    popi_seq = [fa_hapvars_seg[\"REF\"][gn].astype(str) if gei == 0 else fa_hapvars_seg[\"ALT\"][gn].astype(str) for gn,gei in enumerate(popi_gen)]\n",
    "    happhy[\"seq\"][pn] = ''.join(str(e) for e in popi_seq)\n",
    "\n",
    "print(pn,\"/\",happhy.shape[0])\n",
    "happhy.to_csv(\"%s/%s_%s.alig_%s.fasta\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)\n",
    "pd.DataFrame({\n",
    "    \"POS\" : oc_hapvars_seg[\"POS\"].subset(loc_varbool)[:]  \n",
    "}).to_csv(\"%s/%s_%s.alig_%s.pos\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FASTA *Rdl* core hap\n",
    "\n",
    "Haplotype region around 296th codon only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_name        = \"corehap\" \n",
    "loc_varbool        = np.logical_and(oc_hapvars_seg[\"POS\"][:] >= loc_vari - fbp_hap, oc_hapvars_seg[\"POS\"][:] <= loc_vari + fbp_hap)\n",
    "fa_haploty_hap_seg = oc_haploty_hap_seg.compress(loc_varbool)\n",
    "fa_hapvars_seg     = oc_hapvars_seg.compress(loc_varbool)\n",
    "print(export_name,fa_haploty_hap_seg.shape, fa_hapvars_seg.shape)\n",
    "\n",
    "for pn,popi in enumerate(oc_sampleh[\"ox_code\"]):\n",
    "    \n",
    "    if pn % int(happhy.shape[0]/10) == 0 : print(pn,\"/\",happhy.shape[0])\n",
    "    popi_gen = np.ndarray.tolist(fa_haploty_hap_seg[:,pn])\n",
    "    popi_seq = [fa_hapvars_seg[\"REF\"][gn].astype(str) if gei == 0 else fa_hapvars_seg[\"ALT\"][gn].astype(str) for gn,gei in enumerate(popi_gen)]\n",
    "    happhy[\"seq\"][pn] = ''.join(str(e) for e in popi_seq)\n",
    "\n",
    "print(pn,\"/\",happhy.shape[0])\n",
    "happhy.to_csv(\"%s/%s_%s.alig_%s.fasta\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)\n",
    "pd.DataFrame({\n",
    "    \"POS\" : oc_hapvars_seg[\"POS\"].subset(loc_varbool)[:]  \n",
    "}).to_csv(\"%s/%s_%s.alig_%s.pos\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_name        = \"corehap_strict\" \n",
    "loc_varbool        = np.logical_and(oc_hapvars_seg[\"POS\"][:] >= 25428999, oc_hapvars_seg[\"POS\"][:] <= 25434343)\n",
    "fa_haploty_hap_seg = oc_haploty_hap_seg.compress(loc_varbool)\n",
    "fa_hapvars_seg     = oc_hapvars_seg.compress(loc_varbool)\n",
    "print(export_name,fa_haploty_hap_seg.shape, fa_hapvars_seg.shape)\n",
    "\n",
    "for pn,popi in enumerate(oc_sampleh[\"ox_code\"]):\n",
    "    \n",
    "    if pn % int(happhy.shape[0]/10) == 0 : print(pn,\"/\",happhy.shape[0])\n",
    "    popi_gen = np.ndarray.tolist(fa_haploty_hap_seg[:,pn])\n",
    "    popi_seq = [fa_hapvars_seg[\"REF\"][gn].astype(str) if gei == 0 else fa_hapvars_seg[\"ALT\"][gn].astype(str) for gn,gei in enumerate(popi_gen)]\n",
    "    happhy[\"seq\"][pn] = ''.join(str(e) for e in popi_seq)\n",
    "\n",
    "print(pn,\"/\",happhy.shape[0])\n",
    "happhy.to_csv(\"%s/%s_%s.alig_%s.fasta\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)\n",
    "pd.DataFrame({\n",
    "    \"POS\" : oc_hapvars_seg[\"POS\"].subset(loc_varbool)[:]  \n",
    "}).to_csv(\"%s/%s_%s.alig_%s.pos\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FASTA *Rdl* 5' and 3' ends\n",
    "\n",
    "Haplotype alignemnt in the 5' and 3' region of the gene (gene start and end +/- 10kbp):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_name        = \"rdl5p\" \n",
    "loc_varbool        = np.logical_and(oc_hapvars_seg[\"POS\"][:] >= loc_start - fbp_hap, oc_hapvars_seg[\"POS\"][:] <= loc_start + fbp_hap)\n",
    "fa_haploty_hap_seg = oc_haploty_hap_seg.compress(loc_varbool)\n",
    "fa_hapvars_seg     = oc_hapvars_seg.compress(loc_varbool)\n",
    "print(export_name,fa_haploty_hap_seg.shape, fa_hapvars_seg.shape)\n",
    "\n",
    "for pn,popi in enumerate(oc_sampleh[\"ox_code\"]):\n",
    "    \n",
    "    if pn % int(happhy.shape[0]/10) == 0 : print(pn,\"/\",happhy.shape[0])\n",
    "    popi_gen = np.ndarray.tolist(fa_haploty_hap_seg[:,pn])\n",
    "    popi_seq = [fa_hapvars_seg[\"REF\"][gn].astype(str) if gei == 0 else fa_hapvars_seg[\"ALT\"][gn].astype(str) for gn,gei in enumerate(popi_gen)]\n",
    "    happhy[\"seq\"][pn] = ''.join(str(e) for e in popi_seq)\n",
    "\n",
    "print(pn,\"/\",happhy.shape[0])\n",
    "happhy.to_csv(\"%s/%s_%s.alig_%s.fasta\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)\n",
    "pd.DataFrame({\n",
    "    \"POS\" : oc_hapvars_seg[\"POS\"].subset(loc_varbool)[:]  \n",
    "}).to_csv(\"%s/%s_%s.alig_%s.pos\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_name        = \"rdl3p\" \n",
    "loc_varbool        = np.logical_and(oc_hapvars_seg[\"POS\"][:] >= loc_end - fbp_hap, oc_hapvars_seg[\"POS\"][:] <= loc_end + fbp_hap)\n",
    "fa_haploty_hap_seg = oc_haploty_hap_seg.compress(loc_varbool)\n",
    "fa_hapvars_seg     = oc_hapvars_seg.compress(loc_varbool)\n",
    "print(export_name,fa_haploty_hap_seg.shape, fa_hapvars_seg.shape)\n",
    "\n",
    "for pn,popi in enumerate(oc_sampleh[\"ox_code\"]):\n",
    "    \n",
    "    if pn % int(happhy.shape[0]/10) == 0 : print(pn,\"/\",happhy.shape[0])\n",
    "    popi_gen = np.ndarray.tolist(fa_haploty_hap_seg[:,pn])\n",
    "    popi_seq = [fa_hapvars_seg[\"REF\"][gn].astype(str) if gei == 0 else fa_hapvars_seg[\"ALT\"][gn].astype(str) for gn,gei in enumerate(popi_gen)]\n",
    "    happhy[\"seq\"][pn] = ''.join(str(e) for e in popi_seq)\n",
    "\n",
    "print(pn,\"/\",happhy.shape[0])\n",
    "happhy.to_csv(\"%s/%s_%s.alig_%s.fasta\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)\n",
    "pd.DataFrame({\n",
    "    \"POS\" : oc_hapvars_seg[\"POS\"].subset(loc_varbool)[:]  \n",
    "}).to_csv(\"%s/%s_%s.alig_%s.pos\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FASTA upstream & downstream *Rdl*\n",
    "\n",
    "Alignments of haplotypes outside of the putative admixture region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_name        = \"rdlup\" \n",
    "loc_varbool        = np.logical_and(oc_hapvars_seg[\"POS\"][:] >= loc_start - 1e6, oc_hapvars_seg[\"POS\"][:] <= loc_start - 1e6 + 20e3)\n",
    "fa_haploty_hap_seg = oc_haploty_hap_seg.compress(loc_varbool)\n",
    "fa_hapvars_seg     = oc_hapvars_seg.compress(loc_varbool)\n",
    "print(export_name,fa_haploty_hap_seg.shape, fa_hapvars_seg.shape)\n",
    "\n",
    "for pn,popi in enumerate(oc_sampleh[\"ox_code\"]):\n",
    "    \n",
    "    if pn % int(happhy.shape[0]/10) == 0 : print(pn,\"/\",happhy.shape[0])\n",
    "    popi_gen = np.ndarray.tolist(fa_haploty_hap_seg[:,pn])\n",
    "    popi_seq = [fa_hapvars_seg[\"REF\"][gn].astype(str) if gei == 0 else fa_hapvars_seg[\"ALT\"][gn].astype(str) for gn,gei in enumerate(popi_gen)]\n",
    "    happhy[\"seq\"][pn] = ''.join(str(e) for e in popi_seq)\n",
    "\n",
    "print(pn,\"/\",happhy.shape[0])\n",
    "happhy.to_csv(\"%s/%s_%s.alig_%s.fasta\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)\n",
    "pd.DataFrame({\n",
    "    \"POS\" : oc_hapvars_seg[\"POS\"].subset(loc_varbool)[:]  \n",
    "}).to_csv(\"%s/%s_%s.alig_%s.pos\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_name        = \"rdldo\" \n",
    "loc_varbool        = np.logical_and(oc_hapvars_seg[\"POS\"][:] >= loc_end + 1e6, oc_hapvars_seg[\"POS\"][:] <= loc_end + 1e6 + 20e3)\n",
    "fa_haploty_hap_seg = oc_haploty_hap_seg.compress(loc_varbool)\n",
    "fa_hapvars_seg     = oc_hapvars_seg.compress(loc_varbool)\n",
    "print(export_name,fa_haploty_hap_seg.shape, fa_hapvars_seg.shape)\n",
    "\n",
    "for pn,popi in enumerate(oc_sampleh[\"ox_code\"]):\n",
    "    \n",
    "    if pn % int(happhy.shape[0]/10) == 0 : print(pn,\"/\",happhy.shape[0])\n",
    "    popi_gen = np.ndarray.tolist(fa_haploty_hap_seg[:,pn])\n",
    "    popi_seq = [fa_hapvars_seg[\"REF\"][gn].astype(str) if gei == 0 else fa_hapvars_seg[\"ALT\"][gn].astype(str) for gn,gei in enumerate(popi_gen)]\n",
    "    happhy[\"seq\"][pn] = ''.join(str(e) for e in popi_seq)\n",
    "\n",
    "print(pn,\"/\",happhy.shape[0])\n",
    "happhy.to_csv(\"%s/%s_%s.alig_%s.fasta\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)\n",
    "pd.DataFrame({\n",
    "    \"POS\" : oc_hapvars_seg[\"POS\"].subset(loc_varbool)[:]  \n",
    "}).to_csv(\"%s/%s_%s.alig_%s.pos\" % (outdir,outcode,l_nom,export_name),sep=\"\\n\",index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
